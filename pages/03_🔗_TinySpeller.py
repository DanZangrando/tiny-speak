from __future__ import annotations

import random
import tempfile
from dataclasses import dataclass
from pathlib import Path
from typing import Dict, List, Sequence, Tuple

import streamlit as st
import torch
from PIL import Image
from torchvision import transforms

from components.modern_sidebar import display_modern_sidebar
from models import TinyListener, TinyRecognizer, TinySpeak, TinySpeller
from training.audio_dataset import (
    DEFAULT_AUDIO_SPLIT_RATIOS,
    AudioSample,
    AudioWordDataset,
    build_audio_datasets,
)
from training.config import load_master_dataset_config
from training.visual_dataset import DEFAULT_SPLIT_RATIOS, VisualLetterDataset
from utils import (
    WAV2VEC_DIM,
    WAV2VEC_SR,
    encontrar_device,
    get_default_words,
    load_wav2vec_model,
    plot_logits_native,
    save_waveform_to_audio_file,
)


st.set_page_config(page_title="TinySpeller - Multimodal Bridge", page_icon="üîó", layout="wide")

DEFAULT_SEED = 42
LETTER_TRANSFORM = transforms.Compose([transforms.Resize((64, 64)), transforms.ToTensor()])


def get_active_words() -> List[str]:
    """
    Obtiene el vocabulario activo sin cache para evitar problemas de actualizaci√≥n.
    ‚úÖ CORRECCI√ìN: Removido @st.cache_resource que causaba problemas de detecci√≥n.
    """
    try:
        config = load_master_dataset_config()
        selected = config.get("diccionario_seleccionado") or {}
        words = selected.get("palabras") or []
        if isinstance(words, Sequence) and words:
            # Limpiar y deduplicar palabras
            clean_words = [w.strip().lower() for w in words if w and w.strip()]
            return list(dict.fromkeys(clean_words))
    except Exception as exc:
        st.error(f"Error cargando vocabulario activo: {exc}")
    
    # Fallback a vocabulario por defecto
    default = get_default_words()
    st.warning(f"‚ö†Ô∏è Usando vocabulario por defecto ({len(default)} palabras)")
    return default


@st.cache_resource(show_spinner=False)
def load_audio_index(seed: int = DEFAULT_SEED) -> Tuple[Dict[str, List[Dict]], List[str], str | None]:
    try:
        datasets = build_audio_datasets(seed=seed, split_ratios=DEFAULT_AUDIO_SPLIT_RATIOS)
    except Exception as exc:  # noqa: BLE001
        return {}, [], str(exc)

    index: Dict[str, List[Dict]] = {}
    for split_name, dataset in datasets.items():
        entries: List[Dict] = []
        for sample in dataset.samples:
            entries.append(
                {
                    "word": sample.word,
                    "waveform": sample.waveform.clone(),
                    "duration_ms": sample.duration_ms,
                    "metadata": sample.metadata,
                    "split": split_name,
                }
            )
        index[split_name] = entries

    train_ds = datasets.get("train")
    words = train_ds.words if isinstance(train_ds, AudioWordDataset) else []
    return index, words, None


@st.cache_resource(show_spinner=False)
def load_visual_index(seed: int = DEFAULT_SEED, split: str = "train") -> Tuple[Dict[str, List[str]], str | None]:
    try:
        dataset = VisualLetterDataset(
            split=split,
            augment=False,
            seed=seed,
            split_ratios=DEFAULT_SPLIT_RATIOS,
        )
    except Exception as exc:  # noqa: BLE001
        return {}, str(exc)

    index: Dict[str, List[str]] = {}
    for sample in dataset.samples:
        index.setdefault(sample.letter, []).append(str(sample.path))
    return index, None


@st.cache_resource(show_spinner=False)
def load_multimodal_stack() -> Dict:
    """
    Carga el stack completo de modelos multimodales.
    ‚ö†Ô∏è NOTA: TinyRecognizer est√° congelado en TinySpeller (problema arquitectural)
    """
    device = encontrar_device()
    words = get_active_words()

    # Validar que hay vocabulario
    if not words:
        st.error("‚ùå No se pudo cargar vocabulario activo")
        return {"error": "No vocabulary loaded"}
    
    # Cargar modelos
    wav2vec_model = load_wav2vec_model(device=device)
    tiny_speak = TinySpeak(words=words, hidden_dim=128, num_layers=2, wav2vec_dim=WAV2VEC_DIM).to(device)
    tiny_listener = TinyListener(tiny_speak=tiny_speak, wav2vec_model=wav2vec_model).to(device)
    
    # TinyRecognizer - determinar num_classes basado en vocabulario
    unique_letters = set()
    for word in words:
        unique_letters.update(word.lower())
    num_classes = len(unique_letters)
    
    tiny_recognizer = TinyRecognizer(num_classes=num_classes).to(device)
    tiny_speller = TinySpeller(tiny_recognizer=tiny_recognizer, tiny_speak=tiny_speak).to(device)

    # Modo evaluaci√≥n
    tiny_listener.eval()
    tiny_speller.eval()

    return {
        "device": device,
        "words": words,
        "num_classes": num_classes,
        "unique_letters": sorted(unique_letters),
        "tiny_listener": tiny_listener,
        "tiny_speller": tiny_speller,
        "tiny_recognizer": tiny_recognizer,  # Para an√°lisis
        "tiny_speak": tiny_speak,           # Para an√°lisis
        "image_transform": LETTER_TRANSFORM,
        "vocab_size": len(words),
    }


@dataclass
class MultimodalResult:
    word: str
    audio_prediction: str
    audio_confidence: float
    audio_logits: torch.Tensor
    speller_prediction: str
    speller_confidence: float
    speller_logits: torch.Tensor
    letter_paths: List[str]
    audio_split: str
    audio_duration_ms: float | None


def pick_audio_example(word: str, audio_index: Dict[str, List[Dict]], rng: random.Random) -> Dict | None:
    candidates: List[Dict] = []
    for entries in audio_index.values():
        candidates.extend([entry for entry in entries if entry["word"] == word])
    if not candidates:
        return None
    return rng.choice(candidates)


def pick_letter_sequence(word: str, visual_index: Dict[str, List[str]], rng: random.Random) -> Tuple[List[str], str | None]:
    chosen: List[str] = []
    for letter in word:
        options = visual_index.get(letter)
        if not options:
            return [], letter
        chosen.append(rng.choice(options))
    return chosen, None


def run_multimodal_inference(
    word: str,
    seed: int,
    models: Dict,
    audio_index: Dict[str, List[Dict]],
    visual_index: Dict[str, List[str]],
) -> MultimodalResult:
    rng = random.Random(seed)

    audio_entry = pick_audio_example(word, audio_index, rng)
    if audio_entry is None:
        raise RuntimeError(f"No hay audio disponible para '{word}'.")

    letter_paths, missing_letter = pick_letter_sequence(word, visual_index, rng)
    if missing_letter is not None:
        raise RuntimeError(f"No hay im√°genes registradas para la letra '{missing_letter}'.")

    device = models["device"]
    listener: TinyListener = models["tiny_listener"]
    speller: TinySpeller = models["tiny_speller"]

    waveform: torch.Tensor = audio_entry["waveform"].to(device)
    listener.eval()
    with torch.no_grad():
        audio_logits, _ = listener([waveform])
    audio_probs = torch.softmax(audio_logits, dim=-1).squeeze(0)
    audio_top = torch.argmax(audio_probs).item()

    image_tensors: List[torch.Tensor] = []
    for path in letter_paths:
        image = Image.open(Path(path)).convert("RGB")
        image_tensors.append(models["image_transform"](image))
    sequence = torch.stack(image_tensors).unsqueeze(0).to(device)

    speller.eval()
    with torch.no_grad():
        speller_logits, _ = speller(sequence)
    speller_probs = torch.softmax(speller_logits, dim=-1).squeeze(0)
    speller_top = torch.argmax(speller_probs).item()

    words = models["words"]
    audio_prediction = words[audio_top] if words else "‚Äî"
    speller_prediction = words[speller_top] if words else "‚Äî"

    return MultimodalResult(
        word=word,
        audio_prediction=audio_prediction,
        audio_confidence=float(audio_probs[audio_top].item()),
        audio_logits=audio_logits.squeeze(0).cpu(),
        speller_prediction=speller_prediction,
        speller_confidence=float(speller_probs[speller_top].item()),
        speller_logits=speller_logits.squeeze(0).cpu(),
        letter_paths=letter_paths,
        audio_split=audio_entry["split"],
        audio_duration_ms=audio_entry["duration_ms"],
    )


def render_result(result: MultimodalResult, words: List[str]) -> None:
    st.markdown("### Resultados de inferencia")

    match = result.audio_prediction == result.speller_prediction
    status = "üéØ Ambas modalidades coinciden" if match else "‚ö†Ô∏è Predicciones diferentes"
    st.info(f"{status}: audio ‚Üí **{result.audio_prediction}**, visi√≥n ‚Üí **{result.speller_prediction}**")

    audio_col, vision_col = st.columns(2)
    with audio_col:
        st.markdown("#### üéµ TinyListener")
        st.metric("Predicci√≥n", result.audio_prediction, f"{result.audio_confidence:.2%}")
        st.caption(f"Split seleccionado: {result.audio_split} ¬∑ Duraci√≥n: {result.audio_duration_ms or 0:.0f} ms")
        st.plotly_chart(plot_logits_native(result.audio_logits, words), use_container_width=True)

    with vision_col:
        st.markdown("#### üñºÔ∏è TinySpeller")
        st.metric("Predicci√≥n", result.speller_prediction, f"{result.speller_confidence:.2%}")
        st.plotly_chart(plot_logits_native(result.speller_logits, words), use_container_width=True)

    st.markdown("#### üî§ Secuencia de letras utilizada")
    letter_cols = st.columns(len(result.letter_paths)) if result.letter_paths else []
    for idx, path in enumerate(result.letter_paths):
        with letter_cols[idx]:
            st.image(Image.open(Path(path)), caption=f"Letra {idx + 1}", width=96)

    st.markdown("#### üéß Audio reproducido")
    with tempfile.NamedTemporaryFile(delete=False, suffix=".wav") as tmp_file:
        waveform = result.audio_logits.new_tensor([])  # placeholder for typing
    try:
        # We reuse the cached waveform through the index by re-running pick without randomness.
        # Avoid storing full tensor here to keep cache minimal.
        pass
    finally:
        Path(tmp_file.name).unlink(missing_ok=True)


def render_audio_player(word: str, audio_index: Dict[str, List[Dict]], seed: int) -> None:
    rng = random.Random(seed)
    audio_entry = pick_audio_example(word, audio_index, rng)
    if audio_entry is None:
        st.warning("No se pudo mostrar audio de referencia para esta palabra.")
        return

    with tempfile.NamedTemporaryFile(delete=False, suffix=".wav") as tmp_file:
        waveform: torch.Tensor = audio_entry["waveform"]
        if not save_waveform_to_audio_file(waveform, tmp_file.name, WAV2VEC_SR):
            st.warning("No fue posible renderizar el audio.")
            return
        tmp_path = tmp_file.name

    try:
        with open(tmp_path, "rb") as audio_fp:
            st.audio(audio_fp.read(), format="audio/wav")
    finally:
        Path(tmp_path).unlink(missing_ok=True)


def main() -> None:
    display_modern_sidebar("tiny_speller")
    st.title("üîó TinySpeller ‚Äì Multimodal Bridge")
    st.caption("Valida palabras combinando audio y visi√≥n desde el dataset activo.")

    models = load_multimodal_stack()
    
    # Verificar si hay errores en la carga
    if "error" in models:
        st.error(f"Error cargando modelos: {models['error']}")
        return
        
    audio_index, audio_words, audio_error = load_audio_index()
    visual_index, visual_error = load_visual_index()

    vocab_words = models.get("words", [])
    total_audio = sum(len(entries) for entries in audio_index.values())
    total_visual = sum(len(paths) for paths in visual_index.values())

    # M√©tricas principales
    st.markdown("### üìä Estado del Sistema")
    metrics = st.columns(4)
    metrics[0].metric("üî§ Vocabulario Activo", f"{len(vocab_words)} palabras")
    metrics[1].metric("üéµ Muestras Audio", total_audio)
    metrics[2].metric("üñºÔ∏è Im√°genes Letras", total_visual)
    metrics[3].metric("üíª Dispositivo", str(models.get("device", "N/A")))

    # Informaci√≥n adicional del modelo
    if models.get("unique_letters"):
        detail_cols = st.columns(3)
        detail_cols[0].metric("üî† Letras √önicas", len(models["unique_letters"]))
        detail_cols[1].metric("üèóÔ∏è Clases Visual", models.get("num_classes", 0))
        detail_cols[2].metric("üìö Tama√±o Vocab", models.get("vocab_size", 0))
        
        # Mostrar letras detectadas
        with st.expander("üîç Letras Detectadas en Vocabulario"):
            st.write("**Letras √∫nicas encontradas:**")
            letters_text = " ".join(models["unique_letters"])
            st.code(letters_text, language="text")
    
    # Alertas de problemas
    issues = []
    if not vocab_words:
        issues.append("‚ùå No hay vocabulario cargado")
    if total_audio == 0:
        issues.append("‚ùå No hay muestras de audio")
    if total_visual == 0:
        issues.append("‚ùå No hay im√°genes de letras")
    
    if issues:
        st.error("**Problemas detectados:**")
        for issue in issues:
            st.write(issue)

    if audio_error:
        st.warning(f"‚ö†Ô∏è Audio dataset: {audio_error}")
    if visual_error:
        st.warning(f"‚ö†Ô∏è Visual dataset: {visual_error}")

    # Informaci√≥n sobre problemas arquitecturales
    st.markdown("### ‚ö†Ô∏è Estado Arquitectural")
    with st.expander("üîß Problemas Arquitecturales Conocidos", expanded=False):
        st.markdown("""
        **üö® Problemas Identificados en TinySpeller:**
        
        1. **TinyRecognizer Congelado**: 
           - El backbone visual est√° completamente congelado (`requires_grad=False`)
           - Solo el LSTM de TinySpeak puede aprender
           - Limita drasticamente la capacidad de aprendizaje
        
        2. **Procesamiento Ineficiente**:
           - Loop secuencial por cada letra (ineficiente)
           - No hay procesamiento batch-wise de secuencias
           - Falta de mecanismos de atenci√≥n
        
        3. **Sin Entrenamiento End-to-End**:
           - No existe m√≥dulo de entrenamiento TinySpellerLightning
           - No hay dataset multimodal espec√≠fico
           - Sin m√©tricas de evaluaci√≥n para secuencias
        
        **üí° Soluciones Propuestas:**
        - Arquitectura mejorada con backbone entrenable
        - Encoder de secuencias con BiLSTM/Attention
        - Dataset y pipeline de entrenamiento multimodal
        """)
        
        if st.button("üìñ Ver An√°lisis Completo", help="Abre el an√°lisis t√©cnico detallado"):
            st.info("Consulta `TINY_SPELLER_ANALYSIS.md` para el an√°lisis completo y soluciones propuestas.")

    # Tabs principales
    inference_tab, training_tab = st.tabs(["üß™ Inferencia Actual", "üöÄ Entrenamiento Nuevo"])
    
    with inference_tab:
        st.markdown("### üß™ Experimento Multimodal Actual")
        st.caption("‚ö†Ô∏è **Nota**: Este es el modelo actual con arquitectura problem√°tica")

        if not vocab_words:
            st.error("No hay vocabulario disponible. Configura un diccionario en Dataset Manager.")
            return

        selection_col, seed_col = st.columns([2, 1])
        with selection_col:
            selected_word = st.selectbox("Palabra objetivo", options=vocab_words, index=0)
        with seed_col:
            seed_value = st.number_input("Seed aleatoria", value=DEFAULT_SEED, min_value=0, max_value=10_000, step=1)

        if st.button("Ejecutar inferencia multimodal", type="primary"):
            try:
                result = run_multimodal_inference(selected_word, seed_value, models, audio_index, visual_index)
                st.session_state["tiny_speller_result"] = result
            except Exception as exc:  # noqa: BLE001
                st.error(str(exc))

        if "tiny_speller_result" in st.session_state:
            render_result(st.session_state["tiny_speller_result"], vocab_words)
            st.markdown("---")
            st.markdown("### üéß Escucha r√°pida del audio seleccionado")
            render_audio_player(selected_word, audio_index, seed_value)
    
    with training_tab:
        render_training_tab(models, vocab_words)

def test_image_to_word(models):
    """Test de secuencia de im√°genes a palabra"""
    st.subheader("üñºÔ∏è Test: Secuencia de Letras ‚Üí Palabra")
    
    col1, col2 = st.columns([1, 1])
    
    with col1:
        st.markdown("#### ‚öôÔ∏è Configuraci√≥n")
        
        # Selector de palabra para generar
        word_source = st.radio(
            "Fuente de la palabra:",
            ["Del vocabulario", "Palabra personalizada"]
        )
        
        if word_source == "Del vocabulario":
            target_word = st.selectbox(
                "Selecciona palabra del vocabulario:",
                models['words'][:50]  # Primeras 50 para el selector
            )
        else:
            target_word = st.text_input(
                "Escribe una palabra:",
                value="hola",
                max_chars=10,
                help="Solo letras a-z, m√°ximo 10 caracteres"
            ).lower()
        
        # Validar que solo contenga letras v√°lidas
        if target_word and all(c in LETTERS for c in target_word):
            st.success(f"‚úÖ Palabra v√°lida: **{target_word}** ({len(target_word)} letras)")
            
            if st.button("üé® Generar Secuencia de Letras", type="primary"):
                generate_letter_sequence(target_word, models)
        
        elif target_word:
            st.error(f"‚ùå La palabra contiene caracteres inv√°lidos. Solo usar letras a-z.")
    
    with col2:
        if 'letter_sequence_results' in st.session_state:
            display_sequence_results(st.session_state.letter_sequence_results)

def test_audio_to_word(models):
    """Test de audio directo a palabra"""
    st.subheader("üéµ Test: Audio ‚Üí Palabra Directa")
    
    col1, col2 = st.columns([1, 1])
    
    with col1:
        st.markdown("#### üé§ Input de Audio")
        
        # Opci√≥n 1: Audio grabado
        st.markdown("**Grabaci√≥n directa:**")
        recorded_audio = st.audio_input("Graba una palabra")
        
        # Opci√≥n 2: S√≠ntesis
        st.markdown("**O s√≠ntesis de palabra:**")
        synth_word = st.text_input("Palabra para sintetizar:", value="casa")
        
        col_params1, col_params2 = st.columns(2)
        with col_params1:
            rate = st.slider("Velocidad", 50, 200, 80)
        with col_params2:
            pitch = st.slider("Tono", 0, 100, 50)
        
        if st.button("üîä Sintetizar y Analizar"):
            synthesize_and_analyze_audio(synth_word, rate, pitch, models)
        
        # An√°lisis de audio grabado
        if recorded_audio and st.button("üîç Analizar Grabaci√≥n"):
            analyze_recorded_audio(recorded_audio, models)
    
    with col2:
        if 'audio_results' in st.session_state:
            display_audio_results(st.session_state.audio_results)

def test_multimodal_comparison(models):
    """Comparaci√≥n entre modalidades"""
    st.subheader("‚öñÔ∏è Comparaci√≥n Multimodal")
    
    st.markdown("""
    **Objetivo:** Comparar c√≥mo cada modalidad reconoce la misma palabra
    """)
    
    # Configuraci√≥n
    col1, col2 = st.columns([1, 2])
    
    with col1:
        st.markdown("#### ‚öôÔ∏è Configuraci√≥n del Test")
        
        test_word = st.selectbox(
            "Palabra para test comparativo:",
            models['words'][:30]
        )
        
        modalities_to_test = st.multiselect(
            "Modalidades a comparar:",
            ["üñºÔ∏è Secuencia de Letras", "üéµ Audio Sintetizado", "üé§ Audio Directo"],
            default=["üñºÔ∏è Secuencia de Letras", "üéµ Audio Sintetizado"]
        )
        
        if st.button("üöÄ Ejecutar Comparaci√≥n Multimodal"):
            run_multimodal_comparison(test_word, modalities_to_test, models)
    
    with col2:
        if 'comparison_results' in st.session_state:
            display_comparison_results(st.session_state.comparison_results)

def test_advanced_analysis(models):
    """An√°lisis avanzado del sistema multimodal"""
    st.subheader("üî¨ An√°lisis Avanzado del Sistema")
    
    # An√°lisis de arquitectura
    with st.expander("üèóÔ∏è An√°lisis de Arquitectura Completa"):
        display_architecture_analysis(models)
    
    # An√°lisis de embeddings
    with st.expander("üß† An√°lisis de Espacios de Embeddings"):
        analyze_embedding_spaces(models)
    
    # Benchmark del sistema
    with st.expander("‚ö° Benchmark de Rendimiento"):
        run_performance_benchmark(models)

def generate_letter_sequence(word, models):
    """Genera secuencia de im√°genes para una palabra"""
    try:
        st.info(f"üé® Generando secuencia para: **{word}**")
        
        # Generar imagen para cada letra
        letter_images = []
        letter_tensors = []
        
        # Configuraci√≥n de imagen
        mean = [0.485, 0.456, 0.406]
        std = [0.229, 0.224, 0.225]
        
        transform = Compose([
            Resize((28, 28)),
            ToTensor(),
            Normalize(mean, std)
        ])
        
        for letter in word:
            # Crear imagen de la letra
            img = Image.new('RGB', (28, 28), 'white')
            draw = ImageDraw.Draw(img)
            
            try:
                font = ImageFont.truetype("/usr/share/fonts/truetype/liberation/LiberationSans-Regular.ttf", 20)
            except:
                font = ImageFont.load_default()
            
            # Centrar texto
            bbox = draw.textbbox((0, 0), letter.upper(), font=font)
            text_width = bbox[2] - bbox[0]
            text_height = bbox[3] - bbox[1]
            
            x = (28 - text_width) // 2
            y = (28 - text_height) // 2
            
            draw.text((x, y), letter.upper(), fill='black', font=font)
            
            letter_images.append(img)
            letter_tensors.append(transform(img))
        
        # Crear tensor de secuencia
        sequence_tensor = torch.stack(letter_tensors).unsqueeze(0).to(models['device'])
        
        # Procesar con TinySpeller
        models['tiny_speller'].eval()
        with torch.no_grad():
            logits, hidden_states = models['tiny_speller'](sequence_tensor)
        
        # Guardar resultados
        predicted_idx = logits.argmax(dim=1).item()
        predicted_word = models['words'][predicted_idx]
        confidence = torch.softmax(logits, dim=1).max().item()
        
        results = {
            'target_word': word,
            'predicted_word': predicted_word,
            'confidence': confidence,
            'letter_images': letter_images,
            'logits': logits.cpu(),
            'correct': word == predicted_word
        }
        
        st.session_state.letter_sequence_results = results
        
    except Exception as e:
        st.error(f"‚ùå Error generando secuencia: {str(e)}")

def display_sequence_results(results):
    """Muestra resultados de secuencia de letras"""
    st.markdown("#### üìä Resultados")
    
    # Mostrar secuencia de letras generada
    st.markdown("**üî§ Secuencia Generada:**")
    cols = st.columns(len(results['letter_images']))
    for i, img in enumerate(results['letter_images']):
        with cols[i]:
            st.image(img, caption=f"Letra {i+1}", width=60)
    
    # Resultado de predicci√≥n
    if results['correct']:
        st.success(f"‚úÖ **Correcto!** Predicha: {results['predicted_word']} (Confianza: {results['confidence']:.2%})")
    else:
        st.error(f"‚ùå **Incorrecto.** Esperada: {results['target_word']}, Predicha: {results['predicted_word']} (Confianza: {results['confidence']:.2%})")
    
    # Top 5 predicciones
    probabilities = torch.softmax(results['logits'], dim=1).squeeze().numpy()
    top_indices = np.argsort(probabilities)[::-1][:5]
    
    st.markdown("**üèÜ Top 5 Predicciones:**")
    for i, idx in enumerate(top_indices):
        word = st.session_state.multimodal_models['words'][idx]
        prob = probabilities[idx]
        emoji = "üéØ" if i == 0 else "üìç"
        st.write(f"{emoji} {word} ({prob:.2%})")

def synthesize_and_analyze_audio(word, rate, pitch, models):
    """Sintetiza y analiza audio"""
    try:
        # Sintetizar
        waveform = synthesize_word(word, rate=rate, pitch=pitch)
        
        if waveform is not None:
            # Analizar con TinyListener
            device = models['device']
            waveform_device = waveform.to(device)
            
            models['tiny_listener'].eval()
            with torch.no_grad():
                logits, hidden_states = models['tiny_listener']([waveform_device])
            
            # Guardar resultados
            predicted_idx = logits.argmax(dim=1).item()
            predicted_word = models['words'][predicted_idx]
            confidence = torch.softmax(logits, dim=1).max().item()
            
            results = {
                'target_word': word,
                'predicted_word': predicted_word,
                'confidence': confidence,
                'waveform': waveform,
                'logits': logits.cpu(),
                'correct': word == predicted_word,
                'source': 'synthesis'
            }
            
            st.session_state.audio_results = results
        
        else:
            st.error("‚ùå Error en s√≠ntesis de audio")
    
    except Exception as e:
        st.error(f"‚ùå Error: {str(e)}")

def analyze_recorded_audio(audio_file, models):
    """Analiza audio grabado"""
    try:
        from utils import load_waveform
        
        # Guardar temporalmente
        with tempfile.NamedTemporaryFile(delete=False, suffix='.wav') as tmp_file:
            tmp_file.write(audio_file.read())
            tmp_path = tmp_file.name
        
        waveform = load_waveform(tmp_path, target_sr=WAV2VEC_SR)
        os.unlink(tmp_path)
        
        if waveform is not None:
            # Analizar
            device = models['device']
            waveform_device = waveform.to(device)
            
            models['tiny_listener'].eval()
            with torch.no_grad():
                logits, hidden_states = models['tiny_listener']([waveform_device])
            
            predicted_idx = logits.argmax(dim=1).item()
            predicted_word = models['words'][predicted_idx]
            confidence = torch.softmax(logits, dim=1).max().item()
            
            results = {
                'target_word': 'unknown',
                'predicted_word': predicted_word,
                'confidence': confidence,
                'waveform': waveform,
                'logits': logits.cpu(),
                'correct': None,
                'source': 'recording'
            }
            
            st.session_state.audio_results = results
        
        else:
            st.error("‚ùå Error cargando audio")
    
    except Exception as e:
        st.error(f"‚ùå Error: {str(e)}")

def display_audio_results(results):
    """Muestra resultados de an√°lisis de audio"""
    st.markdown("#### üéß Resultados de Audio")
    
    # Reproducir audio
    if results['source'] == 'synthesis':
        try:
            with tempfile.NamedTemporaryFile(delete=False, suffix='.wav') as tmp_file:
                if save_waveform_to_audio_file(results['waveform'], tmp_file.name, WAV2VEC_SR):
                    with open(tmp_file.name, 'rb') as audio_file:
                        st.audio(audio_file.read(), format='audio/wav')
                else:
                    st.warning("‚ö†Ô∏è No se pudo guardar el archivo de audio")
                
                os.unlink(tmp_file.name)
        except Exception as e:
            st.warning(f"‚ö†Ô∏è No se puede reproducir el audio: {str(e)}")
    
    # Resultados
    if results['correct'] is not None:
        if results['correct']:
            st.success(f"‚úÖ **Correcto!** {results['predicted_word']} (Confianza: {results['confidence']:.2%})")
        else:
            st.warning(f"‚ö†Ô∏è Esperada: {results['target_word']}, Predicha: {results['predicted_word']} ({results['confidence']:.2%})")
    else:
        st.info(f"üéØ **Predicci√≥n:** {results['predicted_word']} (Confianza: {results['confidence']:.2%})")
    
    # Waveform
    from utils import plot_waveform
    fig = plot_waveform(results['waveform'], "Audio Analizado")
    st.pyplot(fig)

def run_multimodal_comparison(word, modalities, models):
    """Ejecuta comparaci√≥n entre modalidades"""
    st.info(f"üîÑ Ejecutando comparaci√≥n para: **{word}**")
    
    results = {'word': word, 'comparisons': {}}
    
    # Test de secuencia de letras
    if "üñºÔ∏è Secuencia de Letras" in modalities:
        try:
            generate_letter_sequence(word, models)
            if 'letter_sequence_results' in st.session_state:
                seq_results = st.session_state.letter_sequence_results
                results['comparisons']['vision'] = {
                    'predicted': seq_results['predicted_word'],
                    'confidence': seq_results['confidence'],
                    'correct': seq_results['correct']
                }
        except Exception as e:
            results['comparisons']['vision'] = {'error': str(e)}
    
    # Test de audio sintetizado
    if "üéµ Audio Sintetizado" in modalities:
        try:
            synthesize_and_analyze_audio(word, 80, 50, models)
            if 'audio_results' in st.session_state:
                audio_results = st.session_state.audio_results
                results['comparisons']['audio_synth'] = {
                    'predicted': audio_results['predicted_word'],
                    'confidence': audio_results['confidence'],
                    'correct': audio_results['correct']
                }
        except Exception as e:
            results['comparisons']['audio_synth'] = {'error': str(e)}
    
    st.session_state.comparison_results = results

def display_comparison_results(results):
    """Muestra resultados de comparaci√≥n multimodal"""
    st.markdown("#### üìä Resultados de Comparaci√≥n")
    
    word = results['word']
    comparisons = results['comparisons']
    
    # Tabla comparativa
    st.markdown(f"**Palabra objetivo:** {word}")
    
    for modality, result in comparisons.items():
        if 'error' in result:
            st.error(f"‚ùå {modality}: Error - {result['error']}")
        else:
            status = "‚úÖ" if result['correct'] else "‚ùå"
            st.write(f"{status} **{modality}**: {result['predicted']} (conf: {result['confidence']:.2%})")
    
    # An√°lisis conjunto
    if len(comparisons) > 1:
        predictions = [r['predicted'] for r in comparisons.values() if 'predicted' in r]
        if len(set(predictions)) == 1:
            st.success("üéâ **Consenso:** Todas las modalidades coinciden!")
        else:
            st.warning("‚ö†Ô∏è **Discrepancia:** Las modalidades difieren en la predicci√≥n")

def display_architecture_analysis(models):
    """Muestra an√°lisis detallado de arquitectura"""
    st.markdown("### üèóÔ∏è An√°lisis Completo del Sistema")
    
    # Componentes del sistema
    components = {
        'TinyRecognizer (CORnet-Z)': models['tiny_recognizer'],
        'TinySpeak (LSTM)': models['tiny_speak'],  
        'TinyListener (Wav2Vec2+LSTM)': models['tiny_listener'],
        'TinySpeller (Vision+Audio)': models['tiny_speller']
    }
    
    total_params = 0
    
    for name, model in components.items():
        params = sum(p.numel() for p in model.parameters())
        trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)
        total_params += params
        
        st.markdown(f"**{name}:**")
        st.write(f"- Par√°metros totales: {params:,}")
        st.write(f"- Par√°metros entrenables: {trainable:,}")
        st.write(f"- Par√°metros congelados: {params - trainable:,}")
        st.write("")
    
    st.metric("üß† Sistema Completo", f"{total_params:,} par√°metros")

def analyze_embedding_spaces(models):
    """Analiza espacios de embeddings"""
    st.markdown("### üß† An√°lisis de Espacios de Embeddings")
    
    # Por ahora informaci√≥n te√≥rica
    st.markdown("""
    **Espacios de representaci√≥n en TinySpeak:**
    
    1. **Wav2Vec2 Features (768D)**: Caracter√≠sticas ac√∫sticas del audio
    2. **CORnet-Z Features (768D)**: Caracter√≠sticas visuales de letras  
    3. **LSTM Hidden States (64D)**: Estados internos de secuencia
    4. **Word Embeddings**: Espacio de palabras del vocabulario
    
    **Hip√≥tesis:** Los embeddings de audio y visi√≥n deber√≠an ser similares 
    para la misma letra/palabra, permitiendo transferencia entre modalidades.
    """)
    
    if st.button("üî¨ Analizar Embeddings de Ejemplo"):
        # Crear ejemplo simple
        letter = 'a'
        
        # Embedding visual
        img = Image.new('RGB', (28, 28), 'white')
        draw = ImageDraw.Draw(img)
        draw.text((8, 5), letter.upper(), fill='black')
        
        transform = Compose([
            Resize((28, 28)),
            ToTensor(),
            Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
        ])
        
        image_tensor = transform(img).unsqueeze(0).to(models['device'])
        
        models['tiny_recognizer'].eval()
        with torch.no_grad():
            _, visual_embed = models['tiny_recognizer'](image_tensor)
        
        st.success(f"‚úÖ Embedding visual para '{letter}': {visual_embed.shape}")
        
        # Mostrar estad√≠sticas b√°sicas
        embed_np = visual_embed.squeeze().cpu().numpy()
        st.write(f"- Media: {embed_np.mean():.4f}")
        st.write(f"- Std: {embed_np.std():.4f}")
        st.write(f"- Min: {embed_np.min():.4f}")
        st.write(f"- Max: {embed_np.max():.4f}")

def run_performance_benchmark(models):
    """Ejecuta benchmark de rendimiento"""
    st.markdown("### ‚ö° Benchmark de Rendimiento")
    
    if st.button("üöÄ Ejecutar Benchmark"):
        import time
        
        # Benchmark de TinyRecognizer
        img = Image.new('RGB', (28, 28), 'white')
        transform = Compose([
            Resize((28, 28)),
            ToTensor(),
            Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
        ])
        image_tensor = transform(img).unsqueeze(0).to(models['device'])
        
        # Tiempo de inferencia visual
        models['tiny_recognizer'].eval()
        start_time = time.time()
        with torch.no_grad():
            for _ in range(100):
                _, _ = models['tiny_recognizer'](image_tensor)
        vision_time = (time.time() - start_time) / 100
        
        # S√≠ntesis de audio para benchmark
        waveform = synthesize_word("test")
        if waveform is not None:
            waveform_device = waveform.to(models['device'])
            
            # Tiempo de inferencia audio
            models['tiny_listener'].eval()
            start_time = time.time()
            with torch.no_grad():
                for _ in range(10):  # Menos iteraciones por ser m√°s lento
                    _, _ = models['tiny_listener']([waveform_device])
            audio_time = (time.time() - start_time) / 10
        else:
            audio_time = None
        
        # Mostrar resultados
        st.success("‚úÖ Benchmark completado!")
        
        col1, col2 = st.columns(2)
        with col1:
            st.metric("üñºÔ∏è Visi√≥n (ms)", f"{vision_time*1000:.2f}")
        with col2:
            if audio_time:
                st.metric("üéµ Audio (ms)", f"{audio_time*1000:.2f}")
            else:
                st.metric("üéµ Audio", "Error")

def render_training_tab(models: Dict, vocab_words: List[str]) -> None:
    """Tab de entrenamiento para TinySpeller con implementaci√≥n completa"""
    
    st.subheader("üöÄ Entrenamiento TinySpeller Multimodal")
    st.markdown("""
    Entrena TinySpeller para reconocer palabras completas a partir de secuencias de im√°genes de letras.
    Combina el poder de TinyRecognizer (visi√≥n) con arquitectura secuencial (LSTM).
    """)
    
    # Verificar prerrequisitos
    if not models.get("tiny_speller"):
        st.error("‚ùå TinySpeller no disponible. Verifica la carga del stack multimodal.")
        return
    
    if not vocab_words:
        st.error("‚ùå No hay vocabulario disponible para entrenamiento.")
        return
    
    # Informaci√≥n del dataset
    info_cols = st.columns(4)
    info_cols[0].metric("üìö Palabras Disponibles", len(vocab_words))
    info_cols[1].metric("üî§ Letras √önicas", len(set("".join(vocab_words))))
    info_cols[2].metric("üìè Palabra M√°s Larga", max(len(word) for word in vocab_words))
    info_cols[3].metric("üìä Promedio Longitud", f"{sum(len(w) for w in vocab_words) / len(vocab_words):.1f}")
    
    # Verificar disponibilidad de datasets
    st.markdown("#### üìã Estado de Datasets")
    
    try:
        from training.visual_dataset import VisualLetterDataset
        
        # Verificar dataset visual intentando crear un dataset peque√±o
        try:
            train_dataset = VisualLetterDataset(split="train")
            val_dataset = VisualLetterDataset(split="val")
            test_dataset = VisualLetterDataset(split="test")
            
            has_visual = len(train_dataset) > 0
            visual_splits = {
                'train': len(train_dataset),
                'val': len(val_dataset),
                'test': len(test_dataset)
            }
        except (ValueError, FileNotFoundError, Exception) as e:
            has_visual = False
            visual_splits = {'train': 0, 'val': 0, 'test': 0}
        
        # Verificar dataset audio (basado en generated_samples en config)
        from training.config import load_master_dataset_config
        config = load_master_dataset_config()
        has_audio = len(config.get('generated_samples', {})) > 0
        
        status_cols = st.columns(2)
        with status_cols[0]:
            if has_visual:
                st.success("‚úÖ Dataset Visual Disponible")
                if visual_splits:
                    st.caption(f"Train: {visual_splits['train']}, Val: {visual_splits['val']}, Test: {visual_splits['test']}")
            else:
                st.error("‚ùå Dataset Visual No Disponible")
                st.caption("Ve a üñºÔ∏è Visual Dataset Manager para generar im√°genes")
        
        with status_cols[1]:
            if has_audio:
                st.success("‚úÖ Dataset Audio Disponible")
                st.caption(f"{len(config.get('generated_samples', {}))} palabras con audio")
            else:
                st.error("‚ùå Dataset Audio No Disponible")
                st.caption("Ve a üé§ Audio Dataset Manager para generar audios")
        
        if not has_visual:
            st.warning("‚ö†Ô∏è Dataset visual es necesario para el entrenamiento de TinySpeller.")
            return
            
    except ImportError as e:
        st.error(f"‚ùå Error al importar m√≥dulos de entrenamiento: {e}")
        return
    
    # Formulario de configuraci√≥n de entrenamiento
    st.markdown("#### ‚öôÔ∏è Configuraci√≥n de Entrenamiento")
    
    with st.form("speller_training_form"):
        # Configuraci√≥n b√°sica
        config_cols = st.columns(3)
        
        with config_cols[0]:
            batch_size = st.number_input("Batch Size", min_value=4, max_value=64, value=16, step=4)
            learning_rate = st.number_input("Learning Rate", min_value=1e-5, max_value=1e-2, value=1e-3, format="%.1e")
            max_epochs = st.number_input("√âpocas M√°ximas", min_value=5, max_value=100, value=20)
        
        with config_cols[1]:
            weight_decay = st.number_input("Weight Decay", min_value=0.0, max_value=1e-2, value=1e-4, format="%.1e")
            hidden_dim = st.number_input("Hidden Dimension", min_value=64, max_value=512, value=128, step=32)
            max_word_length = st.number_input("Longitud M√°x. Palabra", min_value=5, max_value=15, value=10)
        
        with config_cols[2]:
            num_workers = st.selectbox("Workers", options=[0, 1, 2, 4], index=0)
            freeze_recognizer = st.checkbox("Congelar TinyRecognizer", value=False, help="Si est√° marcado, solo entrena la parte secuencial")
            label_smoothing = st.number_input("Label Smoothing", min_value=0.0, max_value=0.3, value=0.1)
        
        # Configuraciones avanzadas
        with st.expander("ÔøΩ Configuraci√≥n Avanzada"):
            accelerator = st.selectbox("Acelerador", options=["auto", "cpu", "gpu"], index=0)
            patience = st.number_input("Early Stop Patience", min_value=3, max_value=20, value=5)
            gradient_clip_val = st.number_input("Gradient Clipping", min_value=0.0, max_value=10.0, value=1.0)
        
        # Bot√≥n de entrenamiento
        submitted = st.form_submit_button("üöÄ Iniciar Entrenamiento", type="primary", use_container_width=True)
    
    # Ejecutar entrenamiento
    if submitted:
        with st.spinner("üîÑ Preparando entrenamiento..."):
            try:
                # Filtrar vocabulario por longitud m√°xima
                filtered_vocab = [word for word in vocab_words if len(word) <= max_word_length]
                
                if not filtered_vocab:
                    st.error(f"‚ùå No hay palabras con longitud ‚â§ {max_word_length}")
                    return
                
                st.info(f"üìö Vocabulario filtrado: {len(filtered_vocab)} palabras (de {len(vocab_words)})")
                
                # Importar m√≥dulos de entrenamiento REAL
                try:
                    from training.speller_module import TinySpellerLightning, build_multimodal_dataloaders
                    import pytorch_lightning as pl
                    from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint
                    import torch
                    import os
                    st.success("‚úÖ M√≥dulos de PyTorch Lightning importados correctamente")
                except ImportError as e:
                    st.error(f"‚ùå Error al importar m√≥dulos: {e}")
                    st.info("üí° Instalar dependencias: `pip install pytorch-lightning`")
                    return
                
                # Obtener n√∫mero de letras √∫nicas para num_classes
                unique_letters = sorted(set("".join(filtered_vocab)))
                num_classes = len(unique_letters)
                vocab_size = len(filtered_vocab)
                
                st.info(f"üîß Config: Vocab={vocab_size}, Classes={num_classes}, Letters: {', '.join(unique_letters[:10])}...")
                
                # Crear dataloaders multimodales REALES
                progress_bar = st.progress(0)
                status_text = st.empty()
                
                status_text.text("üìä Creando dataloaders multimodales...")
                progress_bar.progress(0.2)
                
                try:
                    train_loader, val_loader, test_loader = build_multimodal_dataloaders(
                        words=filtered_vocab,
                        batch_size=batch_size,
                        num_workers=num_workers,
                        max_word_length=max_word_length
                    )
                    st.success(f"‚úÖ Dataloaders: Train={len(train_loader)}, Val={len(val_loader)}, Test={len(test_loader)} batches")
                    progress_bar.progress(0.4)
                except Exception as e:
                    st.error(f"‚ùå Error creando dataloaders: {e}")
                    st.info("Verifica que los datasets est√©n disponibles en Visual Dataset Manager")
                    return
                
                # Inicializar modelo TinySpeller Lightning
                status_text.text("ü§ñ Inicializando TinySpellerLightning...")
                progress_bar.progress(0.5)
                
                model = TinySpellerLightning(
                    vocab_size=vocab_size,
                    num_classes=num_classes,
                    hidden_dim=hidden_dim,
                    learning_rate=learning_rate,
                    weight_decay=weight_decay,
                    freeze_recognizer=freeze_recognizer,
                    label_smoothing=label_smoothing
                )
                
                param_count = sum(p.numel() for p in model.parameters())
                trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
                st.success(f"‚úÖ Modelo: {param_count:,} par√°metros ({trainable_params:,} entrenables)")
                progress_bar.progress(0.6)
                
                # Configurar callbacks de PyTorch Lightning
                os.makedirs('checkpoints/speller', exist_ok=True)
                callbacks = [
                    EarlyStopping(
                        monitor='val_loss',
                        patience=patience,
                        verbose=True,
                        mode='min'
                    ),
                    ModelCheckpoint(
                        dirpath='checkpoints/speller',
                        filename='tiny_speller_{epoch:02d}_{val_acc:.2f}',
                        monitor='val_acc',
                        mode='max',
                        save_top_k=2,
                        save_last=True
                    )
                ]
                
                # Configurar PyTorch Lightning Trainer
                status_text.text("‚ö° Configurando Trainer...")
                progress_bar.progress(0.7)
                
                trainer = pl.Trainer(
                    max_epochs=max_epochs,
                    accelerator=accelerator,
                    callbacks=callbacks,
                    gradient_clip_val=gradient_clip_val if gradient_clip_val > 0 else None,
                    enable_progress_bar=False,  # Usamos Streamlit progress
                    enable_model_summary=True,
                    deterministic=False,
                    logger=False  # Sin logging para Streamlit
                )
                
                st.success("‚úÖ Trainer configurado con early stopping y checkpoints")
                progress_bar.progress(0.8)
                
                # EJECUTAR ENTRENAMIENTO REAL
                status_text.text("üöÄ Iniciando entrenamiento PyTorch Lightning...")
                progress_bar.progress(0.9)
                
                with st.spinner("üî• Entrenando TinySpeller..."):
                    # Ejecutar entrenamiento real
                    trainer.fit(model, train_loader, val_loader)
                
                progress_bar.progress(1.0)
                status_text.text("‚úÖ Entrenamiento completado")
                
                # Mostrar resultados simulados
                st.success("üéâ ¬°Entrenamiento completado exitosamente!")
                
                # Obtener m√©tricas finales REALES
                best_val_loss = trainer.callback_metrics.get('val_loss', 0.0)
                best_val_acc = trainer.callback_metrics.get('val_acc', 0.0)
                
                # Evaluar en test set
                test_results = None
                if test_loader and len(test_loader) > 0:
                    status_text.text("üìä Evaluando en test set...")
                    test_results = trainer.test(model, test_loader, verbose=False)
                    test_acc = test_results[0].get('test_acc', 0.0) if test_results else 0.0
                else:
                    test_acc = 0.0
                
                result_cols = st.columns(3)
                result_cols[0].metric("Val Loss", f"{float(best_val_loss):.3f}")
                result_cols[1].metric("Val Accuracy", f"{float(best_val_acc)*100:.1f}%")
                result_cols[2].metric("Test Accuracy", f"{float(test_acc)*100:.1f}%" if test_acc > 0 else "N/A")
                
                # Informaci√≥n del checkpoint
                if trainer.checkpoint_callback and trainer.checkpoint_callback.best_model_path:
                    st.info(f"üíæ **Mejor modelo guardado:** `{trainer.checkpoint_callback.best_model_path}`")
                

                st.info("""
                ÔøΩ **Nota**: Este es un entrenamiento simulado para demostrar la interfaz.
                
                **Para implementaci√≥n real:**
                1. Usar `training.speller_module.build_multimodal_dataloaders()`
                2. Crear `TinySpellerLightning` con configuraci√≥n especificada
                3. Ejecutar entrenamiento real con PyTorch Lightning
                4. Guardar checkpoints y m√©tricas reales
                """)
                
                # Almacenar resultados REALES en session state
                st.session_state['speller_training_result'] = {
                    'vocab': filtered_vocab,
                    'config': {
                        'batch_size': batch_size,
                        'learning_rate': learning_rate,
                        'hidden_dim': hidden_dim,
                        'max_epochs': max_epochs,
                        'num_classes': num_classes,
                        'vocab_size': vocab_size,
                        'freeze_recognizer': freeze_recognizer
                    },
                    'final_metrics': {
                        'val_loss': float(best_val_loss),
                        'val_accuracy': float(best_val_acc) * 100,
                        'test_accuracy': float(test_acc) * 100 if test_acc > 0 else None,
                        'epochs_trained': trainer.current_epoch + 1,
                        'checkpoint_path': trainer.checkpoint_callback.best_model_path if trainer.checkpoint_callback else None
                    },
                    'training_type': 'real_pytorch_lightning'
                }
                
            except Exception as e:
                st.error(f"‚ùå Error al preparar entrenamiento: {str(e)}")
                st.exception(e)
    
    # Mostrar resultados previos si existen
    if 'speller_training_result' in st.session_state:
        st.markdown("#### üìä √öltimo Entrenamiento")
        result = st.session_state['speller_training_result']
        config = result['config']
        metrics = result['final_metrics']
        
        result_cols = st.columns(4)
        result_cols[0].metric("Vocabulario", config['vocab_size'])
        
        # Manejar tanto resultados simulados como reales
        training_type = result.get('training_type', 'simulated')
        if training_type == 'real_pytorch_lightning':
            # Resultados reales
            result_cols[1].metric("Val Accuracy", f"{metrics.get('val_accuracy', 0):.1f}%")
            result_cols[2].metric("Test Accuracy", f"{metrics.get('test_accuracy', 0):.1f}%" if metrics.get('test_accuracy') else "N/A")
            result_cols[3].metric("Val Loss", f"{metrics.get('val_loss', 0):.3f}")
            
            # Mostrar informaci√≥n adicional
            if metrics.get('checkpoint_path'):
                st.info(f"üíæ **Checkpoint guardado:** `{metrics['checkpoint_path']}`")
            st.caption(f"üèÉ Entrenamiento PyTorch Lightning - √âpocas: {metrics.get('epochs_trained', 'N/A')}")
        else:
            # Resultados simulados (compatibilidad hacia atr√°s)
            result_cols[1].metric("Accuracy", f"{metrics.get('accuracy', 0):.1f}%")
            result_cols[2].metric("Top-3 Acc", f"{metrics.get('top3_accuracy', 0):.1f}%")
            result_cols[3].metric("Loss", f"{metrics.get('loss', 0):.3f}")
            st.caption("üìù Resultado de entrenamiento simulado")
        
        if st.button("üóëÔ∏è Limpiar Resultados"):
            del st.session_state['speller_training_result']
            st.rerun()


if __name__ == "__main__":
    main()