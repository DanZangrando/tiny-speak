"""
TinySpeak - Aplicaci√≥n de Reconocimiento de Voz y Visi√≥n
"""
import streamlit as st
import torch
import numpy as np
from PIL import Image
import matplotlib.pyplot as plt
import plotly.graph_objects as go
import plotly.express as px
from pathlib import Path
import io
import tempfile
import os

# Configurar la p√°gina
st.set_page_config(
    page_title="TinySpeak - Reconocimiento Multimodal",
    page_icon="üé§",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Importar nuestros m√≥dulos
from models import TinySpeak, TinyListener, TinyRecognizer, TinySpeller
from utils import (
    encontrar_device, load_wav2vec_model, load_waveform, plot_waveform, 
    plot_logits, ensure_data_downloaded, get_default_words, synthesize_word,
    WAV2VEC_SR, WAV2VEC_DIM, LETTERS
)

# Configuraci√≥n de la aplicaci√≥n
@st.cache_resource
def setup_models():
    """Inicializa los modelos y configuraci√≥n"""
    device = encontrar_device()
    st.sidebar.info(f"Dispositivo detectado: {device}")
    
    # Cargar modelo Wav2Vec2
    with st.spinner("Cargando modelo Wav2Vec2..."):
        wav2vec_model = load_wav2vec_model(device=device)
    
    # Obtener palabras por defecto
    words = get_default_words()
    
    # Inicializar modelos
    tiny_speak = TinySpeak(words=words, hidden_dim=64, num_layers=2, wav2vec_dim=WAV2VEC_DIM)
    tiny_listener = TinyListener(tiny_speak=tiny_speak, wav2vec_model=wav2vec_model)
    tiny_recognizer = TinyRecognizer(wav2vec_dim=WAV2VEC_DIM)
    tiny_speller = TinySpeller(tiny_recognizer=tiny_recognizer, tiny_speak=tiny_speak)
    
    # Mover modelos al dispositivo
    tiny_speak = tiny_speak.to(device)
    tiny_listener = tiny_listener.to(device)
    tiny_recognizer = tiny_recognizer.to(device)
    tiny_speller = tiny_speller.to(device)
    
    return {
        'device': device,
        'wav2vec_model': wav2vec_model,
        'tiny_speak': tiny_speak,
        'tiny_listener': tiny_listener,
        'tiny_recognizer': tiny_recognizer,
        'tiny_speller': tiny_speller,
        'words': words
    }

def main():
    st.title("üé§ TinySpeak - Reconocimiento Multimodal")
    st.markdown("""
    Esta aplicaci√≥n demuestra tres modelos de IA para reconocimiento:
    - **TinyListener**: Reconocimiento de palabras a partir de audio
    - **TinyRecognizer**: Reconocimiento de letras escritas a mano
    - **TinySpeller**: Combinaci√≥n de visi√≥n y audio para deletrear palabras
    """)
    
    # Inicializar modelos
    if 'models' not in st.session_state:
        with st.spinner("Inicializando modelos..."):
            st.session_state.models = setup_models()
    
    models = st.session_state.models
    
    # Sidebar para configuraci√≥n
    st.sidebar.header("‚öôÔ∏è Configuraci√≥n")
    
    # Selector de modelo
    model_choice = st.sidebar.selectbox(
        "Selecciona el modelo a usar:",
        ["TinyListener (Audio ‚Üí Palabra)", "TinyRecognizer (Imagen ‚Üí Letra)", "S√≠ntesis de voz"]
    )
    
    if model_choice == "TinyListener (Audio ‚Üí Palabra)":
        audio_recognition_interface(models)
    elif model_choice == "TinyRecognizer (Imagen ‚Üí Letra)":
        image_recognition_interface(models)
    elif model_choice == "S√≠ntesis de voz":
        speech_synthesis_interface(models)

def audio_recognition_interface(models):
    """Interfaz para reconocimiento de audio"""
    st.header("üéµ Reconocimiento de Audio - TinyListener")
    
    col1, col2 = st.columns([1, 1])
    
    with col1:
        st.subheader("üìÅ Cargar Audio")
        
        # Opci√≥n 1: Subir archivo
        audio_file = st.file_uploader(
            "Sube un archivo de audio:", 
            type=['wav', 'mp3', 'flac', 'm4a']
        )
        
        # Opci√≥n 2: Grabar audio
        st.markdown("**O graba audio directamente:**")
        recorded_audio = st.audio_input("Graba tu voz")
        
        # Procesamiento del audio
        audio_data = None
        if audio_file is not None:
            audio_data = audio_file
            st.success("‚úÖ Archivo de audio cargado")
        elif recorded_audio is not None:
            audio_data = recorded_audio
            st.success("‚úÖ Audio grabado")
        
        if audio_data is not None:
            # Reproducir audio
            st.audio(audio_data)
            
            # Procesar audio
            if st.button("üîç Analizar Audio", type="primary"):
                with st.spinner("Procesando audio..."):
                    try:
                        # Guardar temporalmente
                        with tempfile.NamedTemporaryFile(delete=False, suffix='.wav') as tmp_file:
                            tmp_file.write(audio_data.read())
                            tmp_path = tmp_file.name
                        
                        # Cargar y procesar waveform
                        waveform = load_waveform(tmp_path, target_sr=WAV2VEC_SR)
                        os.unlink(tmp_path)  # Limpiar archivo temporal
                        
                        if waveform is not None:
                            # Mostrar waveform
                            fig = plot_waveform(waveform, "Audio cargado")
                            st.pyplot(fig)
                            
                            # Hacer predicci√≥n
                            device = models['device']
                            waveform = waveform.unsqueeze(0).to(device)
                            
                            models['tiny_listener'].eval()
                            with torch.no_grad():
                                logits, _ = models['tiny_listener']([waveform.squeeze(0)])
                            
                            # Mostrar resultados
                            with col2:
                                st.subheader("üìä Resultados")
                                
                                # Predicci√≥n principal
                                predicted_idx = logits.argmax(dim=1).item()
                                predicted_word = models['words'][predicted_idx]
                                confidence = torch.softmax(logits, dim=1).max().item()
                                
                                st.metric(
                                    label="Palabra Predicha", 
                                    value=predicted_word,
                                    help=f"Confianza: {confidence:.2%}"
                                )
                                
                                # Top 5 predicciones
                                st.subheader("üèÜ Top 5 Predicciones")
                                probabilities = torch.softmax(logits, dim=1).squeeze().cpu().numpy()
                                top_indices = np.argsort(probabilities)[::-1][:5]
                                
                                for i, idx in enumerate(top_indices):
                                    word = models['words'][idx]
                                    prob = probabilities[idx]
                                    st.write(f"{i+1}. **{word}** ({prob:.2%})")
                                
                                # Gr√°fico de logits
                                fig = plot_logits(logits.squeeze().cpu().numpy(), models['words'], "Distribuci√≥n de Predicciones")
                                st.pyplot(fig)
                        
                        else:
                            st.error("‚ùå Error al cargar el archivo de audio")
                    
                    except Exception as e:
                        st.error(f"‚ùå Error procesando audio: {str(e)}")
    
    # Informaci√≥n del modelo
    with st.expander("‚ÑπÔ∏è Informaci√≥n del Modelo"):
        st.markdown("""
        **TinyListener** utiliza:
        - Modelo Wav2Vec2 preentrenado para extraer caracter√≠sticas del audio
        - Red LSTM para procesar secuencias temporales
        - Clasificador lineal para predecir palabras
        
        **Palabras reconocidas:** """ + ", ".join(models['words'][:10]) + "...")

def image_recognition_interface(models):
    """Interfaz para reconocimiento de im√°genes"""
    st.header("üñºÔ∏è Reconocimiento de Letras - TinyRecognizer")
    
    col1, col2 = st.columns([1, 1])
    
    with col1:
        st.subheader("üìÅ Cargar Imagen")
        
        # Opci√≥n 1: Subir archivo
        image_file = st.file_uploader(
            "Sube una imagen de una letra:", 
            type=['png', 'jpg', 'jpeg', 'bmp']
        )
        
        # Opci√≥n 2: Dibujar letra
        st.markdown("**O dibuja una letra:**")
        
        # Canvas para dibujar (simulado con texto por ahora)
        st.info("üñäÔ∏è Funcionalidad de dibujo en desarrollo. Por favor, sube una imagen.")
        
        if image_file is not None:
            # Cargar y mostrar imagen
            image = Image.open(image_file).convert('RGB')
            st.image(image, caption="Imagen cargada", use_column_width=True)
            
            if st.button("üîç Reconocer Letra", type="primary"):
                with st.spinner("Procesando imagen..."):
                    try:
                        # Preprocesar imagen
                        from torchvision.transforms import Compose, ToTensor, Resize, Normalize
                        
                        mean = [0.485, 0.456, 0.406]
                        std = [0.229, 0.224, 0.225]
                        
                        transform = Compose([
                            Resize((28, 28)),
                            ToTensor(),
                            Normalize(mean, std)
                        ])
                        
                        # Convertir imagen
                        image_tensor = transform(image).unsqueeze(0).to(models['device'])
                        
                        # Hacer predicci√≥n
                        models['tiny_recognizer'].eval()
                        with torch.no_grad():
                            logits, embeddings = models['tiny_recognizer'](image_tensor)
                        
                        # Mostrar resultados
                        with col2:
                            st.subheader("üìä Resultados")
                            
                            # Predicci√≥n principal
                            predicted_idx = logits.argmax(dim=1).item()
                            predicted_letter = LETTERS[predicted_idx]
                            confidence = torch.softmax(logits, dim=1).max().item()
                            
                            st.metric(
                                label="Letra Predicha", 
                                value=predicted_letter.upper(),
                                help=f"Confianza: {confidence:.2%}"
                            )
                            
                            # Top 5 predicciones
                            st.subheader("üèÜ Top 5 Predicciones")
                            probabilities = torch.softmax(logits, dim=1).squeeze().cpu().numpy()
                            top_indices = np.argsort(probabilities)[::-1][:5]
                            
                            for i, idx in enumerate(top_indices):
                                letter = LETTERS[idx].upper()
                                prob = probabilities[idx]
                                st.write(f"{i+1}. **{letter}** ({prob:.2%})")
                            
                            # Visualizaci√≥n del embedding
                            st.subheader("üß† Embedding Visual")
                            embedding_2d = embeddings.squeeze().cpu().numpy().reshape(32, 24)
                            
                            fig, ax = plt.subplots(figsize=(8, 6))
                            im = ax.imshow(embedding_2d, cmap='coolwarm')
                            ax.set_title("Representaci√≥n Interna del Modelo")
                            plt.colorbar(im)
                            st.pyplot(fig)
                    
                    except Exception as e:
                        st.error(f"‚ùå Error procesando imagen: {str(e)}")
    
    # Informaci√≥n del modelo
    with st.expander("‚ÑπÔ∏è Informaci√≥n del Modelo"):
        st.markdown("""
        **TinyRecognizer** utiliza:
        - Arquitectura CORnet-Z inspirada en el sistema visual
        - Capas convolucionales para extracci√≥n de caracter√≠sticas
        - Clasificador para reconocer letras del alfabeto (a-z)
        
        **Entrada:** Im√°genes de 28x28 p√≠xeles
        **Salida:** Probabilidades para cada letra del alfabeto
        """)

def speech_synthesis_interface(models):
    """Interfaz para s√≠ntesis de voz"""
    st.header("üîä S√≠ntesis de Voz")
    
    col1, col2 = st.columns([1, 1])
    
    with col1:
        st.subheader("üìù Generar Audio")
        
        # Input de texto
        text_input = st.text_input(
            "Escribe una palabra para sintetizar:",
            value="hola",
            help="Escribe cualquier palabra en espa√±ol"
        )
        
        # Par√°metros de s√≠ntesis
        st.subheader("‚öôÔ∏è Par√°metros de Voz")
        
        col_rate, col_pitch = st.columns(2)
        with col_rate:
            rate = st.slider("Velocidad", 50, 200, 80, help="Velocidad de habla (palabras por minuto)")
        with col_pitch:
            pitch = st.slider("Tono", 0, 100, 50, help="Altura del tono de voz")
        
        amplitude = st.slider("Volumen", 50, 200, 120, help="Amplitud del audio")
        
        if st.button("üéµ Generar Audio", type="primary"):
            if text_input.strip():
                with st.spinner("Generando audio..."):
                    try:
                        # Sintetizar audio
                        waveform = synthesize_word(
                            text_input.strip(),
                            rate=rate,
                            pitch=pitch,
                            amplitude=amplitude
                        )
                        
                        if waveform is not None:
                            with col2:
                                st.subheader("üéß Audio Generado")
                                
                                # Guardar audio temporal para reproducci√≥n
                                with tempfile.NamedTemporaryFile(delete=False, suffix='.wav') as tmp_file:
                                    import torchaudio
                                    torchaudio.save(tmp_file.name, waveform.unsqueeze(0), WAV2VEC_SR)
                                    
                                    # Reproducir audio
                                    with open(tmp_file.name, 'rb') as audio_file:
                                        st.audio(audio_file.read(), format='audio/wav')
                                    
                                    os.unlink(tmp_file.name)  # Limpiar
                                
                                # Mostrar waveform
                                fig = plot_waveform(waveform, f"Audio sintetizado: '{text_input}'")
                                st.pyplot(fig)
                                
                                # An√°lisis con TinyListener
                                st.subheader("üîç An√°lisis con TinyListener")
                                
                                device = models['device']
                                waveform_device = waveform.to(device)
                                
                                models['tiny_listener'].eval()
                                with torch.no_grad():
                                    logits, _ = models['tiny_listener']([waveform_device])
                                
                                # Predicci√≥n
                                predicted_idx = logits.argmax(dim=1).item()
                                predicted_word = models['words'][predicted_idx]
                                confidence = torch.softmax(logits, dim=1).max().item()
                                
                                if predicted_word.lower() == text_input.lower():
                                    st.success(f"‚úÖ ¬°Reconocido correctamente como '{predicted_word}'! (Confianza: {confidence:.2%})")
                                else:
                                    st.warning(f"‚ö†Ô∏è Reconocido como '{predicted_word}' (Confianza: {confidence:.2%})")
                                
                                # Top predicciones
                                probabilities = torch.softmax(logits, dim=1).squeeze().cpu().numpy()
                                top_indices = np.argsort(probabilities)[::-1][:3]
                                
                                st.write("**Top 3 predicciones:**")
                                for i, idx in enumerate(top_indices):
                                    word = models['words'][idx]
                                    prob = probabilities[idx]
                                    icon = "üéØ" if i == 0 else "üìç"
                                    st.write(f"{icon} {word} ({prob:.2%})")
                        
                        else:
                            st.error("‚ùå Error generando audio. ¬øTienes espeak instalado?")
                    
                    except Exception as e:
                        st.error(f"‚ùå Error en s√≠ntesis: {str(e)}")
            else:
                st.warning("‚ö†Ô∏è Por favor ingresa una palabra")
    
    # Informaci√≥n
    with st.expander("‚ÑπÔ∏è Informaci√≥n sobre S√≠ntesis"):
        st.markdown("""
        **S√≠ntesis de Voz** utiliza:
        - **espeak** para generar audio sint√©tico
        - Configuraci√≥n personalizable de velocidad, tono y volumen
        - An√°lisis autom√°tico con TinyListener para verificar calidad
        
        **Nota:** Requiere tener instalado `espeak` en el sistema:
        ```bash
        sudo apt-get install espeak  # Ubuntu/Debian
        brew install espeak         # macOS
        ```
        """)

if __name__ == "__main__":
    main()