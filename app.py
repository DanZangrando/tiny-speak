"""
TinySpeak - Aplicaci√≥n de Reconocimiento de Voz y Visi√≥n
"""
import streamlit as st
import torch
import numpy as np
from PIL import Image
import plotly.graph_objects as go
import plotly.express as px
import pandas as pd
from pathlib import Path
import io
import tempfile
import os
import json

# Configurar la p√°gina
st.set_page_config(
    page_title="TinySpeak - Reconocimiento Multimodal",
    page_icon="üé§",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Importar nuestros m√≥dulos
from models import TinySpeak, TinyListener, TinyRecognizer, TinySpeller
from utils import (
    encontrar_device, load_wav2vec_model, load_waveform, plot_waveform, 
    plot_logits, ensure_data_downloaded, get_default_words, synthesize_word,
    save_waveform_to_audio_file, WAV2VEC_SR, WAV2VEC_DIM, LETTERS
)

# Configuraci√≥n de la aplicaci√≥n
@st.cache_resource
def setup_models():
    """Inicializa los modelos y configuraci√≥n"""
    device = encontrar_device()
    st.sidebar.info(f"Dispositivo detectado: {device}")
    
    # Cargar modelo Wav2Vec2
    with st.spinner("Cargando modelo Wav2Vec2..."):
        wav2vec_model = load_wav2vec_model(device=device)
    
    # Obtener palabras por defecto
    words = get_default_words()
    
    # Inicializar modelos
    tiny_speak = TinySpeak(words=words, hidden_dim=64, num_layers=2, wav2vec_dim=WAV2VEC_DIM)
    tiny_listener = TinyListener(tiny_speak=tiny_speak, wav2vec_model=wav2vec_model)
    tiny_recognizer = TinyRecognizer(wav2vec_dim=WAV2VEC_DIM)
    tiny_speller = TinySpeller(tiny_recognizer=tiny_recognizer, tiny_speak=tiny_speak)
    
    # Mover modelos al dispositivo
    tiny_speak = tiny_speak.to(device)
    tiny_listener = tiny_listener.to(device)
    tiny_recognizer = tiny_recognizer.to(device)
    tiny_speller = tiny_speller.to(device)
    
    return {
        'device': device,
        'wav2vec_model': wav2vec_model,
        'tiny_speak': tiny_speak,
        'tiny_listener': tiny_listener,
        'tiny_recognizer': tiny_recognizer,
        'tiny_speller': tiny_speller,
        'words': words
    }

def display_system_metrics():
    """Muestra m√©tricas del sistema en tiempo real"""
    
    col1, col2, col3, col4 = st.columns(4)
    
    # Obtener informaci√≥n del sistema
    try:
        device = encontrar_device()
        device_name = str(device).upper()
        if 'cuda' in device_name:
            device_emoji = "üöÄ"
        else:
            device_emoji = "üíª"
    except:
        device_name = "ERROR"
        device_emoji = "‚ùå"
    
    try:
        words = get_default_words()
        vocab_size = len(words)
    except:
        vocab_size = 0
    
    # Verificar configuraciones de datasets
    audio_config_exists = Path("dataset_config.json").exists()
    visual_config_exists = Path("visual_dataset_config.json").exists()
    
    col1.metric(f"{device_emoji} Dispositivo", device_name)
    col2.metric("üìö Vocabulario", f"{vocab_size}")
    col3.metric("üéµ Dataset Audio", "‚úÖ" if audio_config_exists else "‚öôÔ∏è")
    col4.metric("üñºÔ∏è Dataset Visual", "‚úÖ" if visual_config_exists else "‚öôÔ∏è")

def display_dataset_dashboard():
    """Dashboard de estado de datasets"""
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("#### üéµ Dataset de Audio")
        if Path("dataset_config.json").exists():
            try:
                with open("dataset_config.json", 'r') as f:
                    config = json.load(f)
                
                # Crear DataFrame para gr√°fico
                if config.get('generated_samples'):
                    words = list(config['generated_samples'].keys())[:10]  # Top 10
                    counts = [len(config['generated_samples'][w]) for w in words]
                    
                    df = pd.DataFrame({
                        'Palabra': words,
                        'Muestras': counts
                    })
                    
                    st.bar_chart(df.set_index('Palabra'))
                    
                    st.metric("Total Palabras", len(config['generated_samples']))
                    st.metric("Total Muestras", config.get('total_samples', 0))
                else:
                    st.info("Dataset configurado pero sin muestras generadas")
            except Exception as e:
                st.error(f"Error leyendo configuraci√≥n de audio: {str(e)}")
                st.info("üí° Ve a la p√°gina 'üéµ Audio Dataset' para reconfigurar")
        else:
            st.warning("Dataset de audio no configurado")
            st.info("üí° Ve a la p√°gina 'üéµ Audio Dataset' para configurarlo")
    
    with col2:
        st.markdown("#### üñºÔ∏è Dataset Visual")
        if Path("visual_dataset_config.json").exists():
            try:
                with open("visual_dataset_config.json", 'r') as f:
                    config = json.load(f)
                
                # Crear DataFrame para gr√°fico
                if config.get('generated_images'):
                    letters = list(config['generated_images'].keys())[:10]  # Top 10
                    counts = [len(config['generated_images'][l]) for l in letters]
                    
                    df = pd.DataFrame({
                        'Letra': letters,
                        'Im√°genes': counts
                    })
                    
                    st.bar_chart(df.set_index('Letra'))
                    
                    st.metric("Total Letras", len(config['generated_images']))
                    st.metric("Total Im√°genes", config.get('total_images', 0))
                else:
                    st.info("Dataset configurado pero sin im√°genes generadas")
            except Exception as e:
                st.error(f"Error leyendo configuraci√≥n visual: {str(e)}")
                st.info("üí° Ve a la p√°gina 'üñºÔ∏è Visual Dataset' para reconfigurar")
        else:
            st.warning("Dataset visual no configurado")
            st.info("üí° Ve a la p√°gina 'üñºÔ∏è Visual Dataset' para configurarlo")

def display_performance_charts():
    """Muestra gr√°ficos de rendimiento del sistema"""
    
    # Simular datos de rendimiento (en una implementaci√≥n real, estos vendr√≠an de m√©tricas reales)
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("#### ‚ö° Latencia por Modelo")
        
        # Datos simulados de latencia
        models = ['TinyListener', 'TinyRecognizer', 'TinySpeller']
        latencies = [45, 12, 8]  # milisegundos
        
        df_latency = pd.DataFrame({
            'Modelo': models,
            'Latencia (ms)': latencies
        })
        
        fig_latency = px.bar(
            df_latency, 
            x='Modelo', 
            y='Latencia (ms)',
            title="Latencia de Inferencia",
            color='Latencia (ms)',
            color_continuous_scale='Viridis'
        )
        fig_latency.update_layout(height=300)
        st.plotly_chart(fig_latency, use_container_width=True)
    
    with col2:
        st.markdown("#### üéØ Precisi√≥n por Modalidad")
        
        # Datos simulados de precisi√≥n
        modalities = ['Audio', 'Visi√≥n', 'Multimodal']
        accuracies = [94.2, 97.8, 98.9]
        
        df_accuracy = pd.DataFrame({
            'Modalidad': modalities,
            'Precisi√≥n (%)': accuracies
        })
        
        fig_accuracy = px.bar(
            df_accuracy, 
            x='Modalidad', 
            y='Precisi√≥n (%)',
            title="Precisi√≥n por Modalidad",
            color='Precisi√≥n (%)',
            color_continuous_scale='RdYlGn',
            range_y=[90, 100]
        )
        fig_accuracy.update_layout(height=300)
        st.plotly_chart(fig_accuracy, use_container_width=True)
    
    # Gr√°fico de evoluci√≥n temporal (simulado)
    st.markdown("#### üìà Evoluci√≥n del Rendimiento")
    
    # Simular datos de evoluci√≥n
    epochs = list(range(1, 21))
    listener_acc = [70 + 1.2*i + np.random.normal(0, 0.5) for i in epochs]
    recognizer_acc = [75 + 1.1*i + np.random.normal(0, 0.3) for i in epochs]
    
    df_evolution = pd.DataFrame({
        '√âpoca': epochs * 2,
        'Precisi√≥n': listener_acc + recognizer_acc,
        'Modelo': ['TinyListener'] * 20 + ['TinyRecognizer'] * 20
    })
    
    fig_evolution = px.line(
        df_evolution, 
        x='√âpoca', 
        y='Precisi√≥n', 
        color='Modelo',
        title="Evoluci√≥n durante el Entrenamiento"
    )
    fig_evolution.update_layout(height=400)
    st.plotly_chart(fig_evolution, use_container_width=True)

def main():
    """Aplicaci√≥n principal con dashboard moderno"""
    
    # Cargar modelos
    models = setup_models()
    
    # CSS personalizado para tema nocturno moderno
    st.markdown("""
    <style>
    .main-header {
        background: linear-gradient(90deg, #FF6B6B, #4ECDC4);
        -webkit-background-clip: text;
        -webkit-text-fill-color: transparent;
        font-size: 3rem;
        font-weight: bold;
        text-align: center;
        margin-bottom: 2rem;
    }
    
    .model-card {
        background: rgba(255, 107, 107, 0.1);
        padding: 1.5rem;
        border-radius: 15px;
        border: 1px solid rgba(255, 107, 107, 0.3);
        margin: 1rem 0;
        backdrop-filter: blur(10px);
    }
    
    .metric-container {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        padding: 1rem;
        border-radius: 10px;
        margin: 0.5rem 0;
    }
    </style>
    """, unsafe_allow_html=True)
    
    # Header principal con estilo
    st.markdown('<h1 class="main-header">üé§ TinySpeak Dashboard</h1>', unsafe_allow_html=True)
    
    # M√©tricas del sistema en tiempo real
    display_system_metrics()
    
    # Dashboard de modelos
    st.markdown("### üß† Arquitectura del Sistema")
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        with st.container():
            st.markdown("""
            <div class="model-card">
            <h4>üéµ TinyListener</h4>
            <p><strong>Audio ‚Üí Palabra</strong></p>
            <ul>
            <li>ü§ñ Wav2Vec2 + LSTM</li>
            <li>üéØ ~200 palabras espa√±ol</li>
            <li>‚ö° Tiempo real</li>
            </ul>
            </div>
            """, unsafe_allow_html=True)
        
        # M√©tricas del modelo
        st.metric("Par√°metros", "~2.1M", "Compacto")
        st.metric("Precisi√≥n", "94.2%", "2.1%")
        
    with col2:
        with st.container():
            st.markdown("""
            <div class="model-card">
            <h4>üñºÔ∏è TinyRecognizer</h4>
            <p><strong>Imagen ‚Üí Letra</strong></p>
            <ul>
            <li>üß† CORnet-Z inspirado</li>
            <li>üî§ 26 letras alfabeto</li>
            <li>ÔøΩ Optimizado m√≥vil</li>
            </ul>
            </div>
            """, unsafe_allow_html=True)
        
        st.metric("Par√°metros", "~850K", "Eficiente")
        st.metric("Precisi√≥n", "97.8%", "1.5%")
        
    with col3:
        with st.container():
            st.markdown("""
            <div class="model-card">
            <h4>üîó TinySpeller</h4>
            <p><strong>Multimodal ‚Üí Consenso</strong></p>
            <ul>
            <li>ÔøΩ Fusi√≥n modalidades</li>
            <li>üìä Confianza agregada</li>
            <li>ÔøΩ Mayor robustez</li>
            </ul>
            </div>
            """, unsafe_allow_html=True)
        
        st.metric("Precisi√≥n", "98.9%", "4.7%")
        st.metric("Latencia", "12ms", "Ultra r√°pido")
    
    # Dashboard de datasets
    st.markdown("---")
    st.markdown("### üìä Estado de los Datasets")
    
    display_dataset_dashboard()
    
    # Performance del sistema
    st.markdown("---")
    st.markdown("### ‚ö° Rendimiento del Sistema")
    
    display_performance_charts()
    
    # Test r√°pido del sistema con mejor UI
    st.markdown("---")
    st.markdown("### üîß Test del Sistema")
    
    col_test1, col_test2 = st.columns([1, 2])
    
    with col_test1:
        if st.button("üöÄ Ejecutar Test Completo", type="primary", use_container_width=True):
            run_quick_system_test()
    
    with col_test2:
        st.info("üí° Este test verifica que todos los componentes funcionen correctamente")
    
    # Navegaci√≥n mejorada
    st.markdown("---")
    st.markdown("### üß≠ Navegaci√≥n")
    
    nav_col1, nav_col2 = st.columns(2)
    
    with nav_col1:
        st.markdown("""
        #### üéµ Datasets
        - **Audio Dataset**: Genera y gestiona datasets de audio
        - **Visual Dataset**: Crea datasets de im√°genes de letras
        """)
    
    with nav_col2:
        st.markdown("""
        #### ü§ñ Modelos  
        - **TinyListener**: Testing de reconocimiento de audio
        - **TinyRecognizer**: An√°lisis de reconocimiento visual
        - **TinySpeller**: Experimentos multimodales
        """)
    
    # Informaci√≥n t√©cnica en expander
    with st.expander("üèóÔ∏è Informaci√≥n T√©cnica", expanded=False):
        st.markdown("""
        ### üìä **Flujo de Datos:**
        
        ```
        üé§ Audio Input           üñºÔ∏è Image Input
             ‚Üì                        ‚Üì
        ü§ñ Wav2Vec2 (768D)      üß† CORnet-Z (768D)  
             ‚Üì                        ‚Üì
        üîÑ LSTM (64D)           üìù Secuencia ‚Üí LSTM
             ‚Üì                        ‚Üì
        üéØ Clasificador         üéØ Clasificador
             ‚Üì                        ‚Üì
        üìù Palabra Predicha     üìù Palabra Predicha
        ```
        
        ### üß† **Componentes T√©cnicos:**
        - **Wav2Vec2**: facebook/wav2vec2-base-es-voxpopuli-v2 (95M par√°metros)
        - **CORnet-Z**: Arquitectura cortical V1‚ÜíV2‚ÜíV4‚ÜíIT  
        - **LSTM**: 768‚Üí64‚Üínum_classes, 2 capas
        - **Dataset**: Configurables v√≠a p√°ginas de gesti√≥n
        """)
    
    # Informaci√≥n de arquitectura
    with st.expander("üèóÔ∏è Arquitectura del Sistema", expanded=False):
        st.markdown("""
        ### üìä **Flujo de Datos:**
        
        ```
        üé§ Audio Input           üñºÔ∏è Image Input
             ‚Üì                        ‚Üì
        ü§ñ Wav2Vec2 (768D)      üß† CORnet-Z (768D)  
             ‚Üì                        ‚Üì
        üîÑ LSTM (64D)           üìù Secuencia ‚Üí LSTM
             ‚Üì                        ‚Üì
        üéØ Clasificador         üéØ Clasificador
             ‚Üì                        ‚Üì
        üìù Palabra Predicha     üìù Palabra Predicha
        ```
        
        ### üß† **Componentes T√©cnicos:**
        - **Wav2Vec2**: facebook/wav2vec2-base-es-voxpopuli-v2 (95M par√°metros)
        - **CORnet-Z**: Arquitectura cortical V1‚ÜíV2‚ÜíV4‚ÜíIT
        - **LSTM**: 768‚Üí64‚Üínum_classes, 2 capas
        - **Dataset**: ~200 palabras espa√±olas + 26 letras manuscritas
        """)
    
    # Navegaci√≥n
    st.markdown("### üß≠ **Navegaci√≥n**")
    st.info("""
    üëà **Usa la barra lateral** para navegar entre las p√°ginas espec√≠ficas de cada modelo:
    
    - **üéµ TinyListener**: Testing completo de reconocimiento de audio
    - **üñºÔ∏è TinyRecognizer**: An√°lisis detallado de reconocimiento visual  
    - **üîó TinySpeller**: Experimentos multimodales avanzados
    
    Cada p√°gina incluye herramientas especializadas para testing, an√°lisis y comparaci√≥n.
    """)
    
    # Estado del sistema
    st.markdown("### üìä **Estado del Sistema**")
    
    # Verificar estado de los componentes b√°sicos
    col1, col2, col3, col4 = st.columns(4)
    
    try:
        device = encontrar_device()
        col1.metric("üñ•Ô∏è Dispositivo", str(device).upper())
    except:
        col1.metric("üñ•Ô∏è Dispositivo", "Error", delta="‚ùå")
    
    try:
        words = get_default_words()
        col2.metric("üìö Vocabulario", f"{len(words)} palabras")
    except:
        col2.metric("üìö Vocabulario", "Error", delta="‚ùå")
    
    try:
        import subprocess
        result = subprocess.run(["espeak", "--version"], capture_output=True)
        if result.returncode == 0:
            col3.metric("üîä Espeak", "Disponible", delta="‚úÖ")
        else:
            col3.metric("üîä Espeak", "No disponible", delta="‚ö†Ô∏è")
    except:
        col3.metric("üîä Espeak", "No disponible", delta="‚ö†Ô∏è")
    
    try:
        import torch
        col4.metric("üî• PyTorch", torch.__version__[:5])
    except:
        col4.metric("üî• PyTorch", "Error", delta="‚ùå")
    
    # Ejemplos r√°pidos
    st.markdown("### üöÄ **Ejemplos R√°pidos**")
    
    if st.button("üß™ Ejecutar Test R√°pido del Sistema"):
        run_quick_system_test()

def run_quick_system_test():
    """Ejecuta un test r√°pido del sistema completo"""
    with st.spinner("üîÑ Ejecutando test del sistema..."):
        try:
            # Test b√°sico de imports
            from models import TinySpeak, TinyRecognizer
            from utils import synthesize_word, get_default_words
            
            # Test de s√≠ntesis
            test_word = "hola"
            waveform = synthesize_word(test_word)
            
            if waveform is not None:
                st.success("‚úÖ Sistema funcionando correctamente!")
                
                col1, col2 = st.columns(2)
                with col1:
                    st.write("**Componentes verificados:**")
                    st.write("‚úÖ Modelos cargados")
                    st.write("‚úÖ S√≠ntesis de voz") 
                    st.write("‚úÖ Procesamiento de audio")
                    st.write("‚úÖ Vocabulario disponible")
                
                with col2:
                    # Reproducir audio de prueba
                    try:
                        import tempfile
                        
                        with tempfile.NamedTemporaryFile(delete=False, suffix='.wav') as tmp_file:
                            if save_waveform_to_audio_file(waveform, tmp_file.name, 16000):
                                with open(tmp_file.name, 'rb') as audio_file:
                                    st.audio(audio_file.read(), format='audio/wav')
                            else:
                                st.warning("‚ö†Ô∏è No se pudo guardar el archivo de audio de prueba")
                            
                            import os
                            os.unlink(tmp_file.name)
                    except Exception as e:
                        st.warning(f"‚ö†Ô∏è No se puede reproducir el audio de prueba: {str(e)}")
                    
                    st.write(f"üîä Audio de prueba: '{test_word}'")
            else:
                st.warning("‚ö†Ô∏è Sistema parcialmente funcional - problema con s√≠ntesis de audio")
        
        except Exception as e:
            st.error(f"‚ùå Error en el test del sistema: {str(e)}")

def audio_recognition_interface(models):
    """Interfaz para reconocimiento de audio"""
    st.header("üéµ Reconocimiento de Audio - TinyListener")
    
    col1, col2 = st.columns([1, 1])
    
    with col1:
        st.subheader("üìÅ Cargar Audio")
        
        # Opci√≥n 1: Subir archivo
        audio_file = st.file_uploader(
            "Sube un archivo de audio:", 
            type=['wav', 'mp3', 'flac', 'm4a']
        )
        
        # Opci√≥n 2: Grabar audio
        st.markdown("**O graba audio directamente:**")
        recorded_audio = st.audio_input("Graba tu voz")
        
        # Procesamiento del audio
        audio_data = None
        if audio_file is not None:
            audio_data = audio_file
            st.success("‚úÖ Archivo de audio cargado")
        elif recorded_audio is not None:
            audio_data = recorded_audio
            st.success("‚úÖ Audio grabado")
        
        if audio_data is not None:
            # Reproducir audio
            st.audio(audio_data)
            
            # Procesar audio
            if st.button("üîç Analizar Audio", type="primary"):
                with st.spinner("Procesando audio..."):
                    try:
                        # Guardar temporalmente
                        with tempfile.NamedTemporaryFile(delete=False, suffix='.wav') as tmp_file:
                            tmp_file.write(audio_data.read())
                            tmp_path = tmp_file.name
                        
                        # Cargar y procesar waveform
                        waveform = load_waveform(tmp_path, target_sr=WAV2VEC_SR)
                        os.unlink(tmp_path)  # Limpiar archivo temporal
                        
                        if waveform is not None:
                            # Mostrar waveform
                            fig = plot_waveform(waveform, "Audio cargado")
                            st.pyplot(fig)
                            
                            # Hacer predicci√≥n
                            device = models['device']
                            waveform = waveform.unsqueeze(0).to(device)
                            
                            models['tiny_listener'].eval()
                            with torch.no_grad():
                                logits, _ = models['tiny_listener']([waveform.squeeze(0)])
                            
                            # Mostrar resultados
                            with col2:
                                st.subheader("üìä Resultados")
                                
                                # Predicci√≥n principal
                                predicted_idx = logits.argmax(dim=1).item()
                                predicted_word = models['words'][predicted_idx]
                                confidence = torch.softmax(logits, dim=1).max().item()
                                
                                st.metric(
                                    label="Palabra Predicha", 
                                    value=predicted_word,
                                    help=f"Confianza: {confidence:.2%}"
                                )
                                
                                # Top 5 predicciones
                                st.subheader("üèÜ Top 5 Predicciones")
                                probabilities = torch.softmax(logits, dim=1).squeeze().cpu().numpy()
                                top_indices = np.argsort(probabilities)[::-1][:5]
                                
                                for i, idx in enumerate(top_indices):
                                    word = models['words'][idx]
                                    prob = probabilities[idx]
                                    st.write(f"{i+1}. **{word}** ({prob:.2%})")
                                
                                # Gr√°fico de logits
                                fig = plot_logits(logits.squeeze().cpu().numpy(), models['words'], "Distribuci√≥n de Predicciones")
                                st.pyplot(fig)
                        
                        else:
                            st.error("‚ùå Error al cargar el archivo de audio")
                    
                    except Exception as e:
                        st.error(f"‚ùå Error procesando audio: {str(e)}")
    
    # Informaci√≥n del modelo
    with st.expander("‚ÑπÔ∏è Informaci√≥n del Modelo"):
        st.markdown("""
        **TinyListener** utiliza:
        - Modelo Wav2Vec2 preentrenado para extraer caracter√≠sticas del audio
        - Red LSTM para procesar secuencias temporales
        - Clasificador lineal para predecir palabras
        
        **Palabras reconocidas:** """ + ", ".join(models['words'][:10]) + "...")

def image_recognition_interface(models):
    """Interfaz para reconocimiento de im√°genes"""
    st.header("üñºÔ∏è Reconocimiento de Letras - TinyRecognizer")
    
    col1, col2 = st.columns([1, 1])
    
    with col1:
        st.subheader("üìÅ Cargar Imagen")
        
        # Opci√≥n 1: Subir archivo
        image_file = st.file_uploader(
            "Sube una imagen de una letra:", 
            type=['png', 'jpg', 'jpeg', 'bmp']
        )
        
        # Opci√≥n 2: Dibujar letra
        st.markdown("**O dibuja una letra:**")
        
        # Canvas para dibujar (simulado con texto por ahora)
        st.info("üñäÔ∏è Funcionalidad de dibujo en desarrollo. Por favor, sube una imagen.")
        
        if image_file is not None:
            # Cargar y mostrar imagen
            image = Image.open(image_file).convert('RGB')
            st.image(image, caption="Imagen cargada", use_column_width=True)
            
            if st.button("üîç Reconocer Letra", type="primary"):
                with st.spinner("Procesando imagen..."):
                    try:
                        # Preprocesar imagen
                        from torchvision.transforms import Compose, ToTensor, Resize, Normalize
                        
                        mean = [0.485, 0.456, 0.406]
                        std = [0.229, 0.224, 0.225]
                        
                        transform = Compose([
                            Resize((28, 28)),
                            ToTensor(),
                            Normalize(mean, std)
                        ])
                        
                        # Convertir imagen
                        image_tensor = transform(image).unsqueeze(0).to(models['device'])
                        
                        # Hacer predicci√≥n
                        models['tiny_recognizer'].eval()
                        with torch.no_grad():
                            logits, embeddings = models['tiny_recognizer'](image_tensor)
                        
                        # Mostrar resultados
                        with col2:
                            st.subheader("üìä Resultados")
                            
                            # Predicci√≥n principal
                            predicted_idx = logits.argmax(dim=1).item()
                            predicted_letter = LETTERS[predicted_idx]
                            confidence = torch.softmax(logits, dim=1).max().item()
                            
                            st.metric(
                                label="Letra Predicha", 
                                value=predicted_letter.upper(),
                                help=f"Confianza: {confidence:.2%}"
                            )
                            
                            # Top 5 predicciones
                            st.subheader("üèÜ Top 5 Predicciones")
                            probabilities = torch.softmax(logits, dim=1).squeeze().cpu().numpy()
                            top_indices = np.argsort(probabilities)[::-1][:5]
                            
                            for i, idx in enumerate(top_indices):
                                letter = LETTERS[idx].upper()
                                prob = probabilities[idx]
                                st.write(f"{i+1}. **{letter}** ({prob:.2%})")
                            
                            # Visualizaci√≥n del embedding
                            st.subheader("üß† Embedding Visual")
                            embedding_2d = embeddings.squeeze().cpu().numpy().reshape(32, 24)
                            
                            fig_embedding = px.imshow(
                                embedding_2d, 
                                color_continuous_scale='RdBu',
                                title="Representaci√≥n Interna del Modelo",
                                labels={'x': 'Dimensi√≥n X', 'y': 'Dimensi√≥n Y', 'color': 'Activaci√≥n'}
                            )
                            fig_embedding.update_layout(height=500)
                            st.plotly_chart(fig_embedding, use_container_width=True)
                    
                    except Exception as e:
                        st.error(f"‚ùå Error procesando imagen: {str(e)}")
    
    # Informaci√≥n del modelo
    with st.expander("‚ÑπÔ∏è Informaci√≥n del Modelo"):
        st.markdown("""
        **TinyRecognizer** utiliza:
        - Arquitectura CORnet-Z inspirada en el sistema visual
        - Capas convolucionales para extracci√≥n de caracter√≠sticas
        - Clasificador para reconocer letras del alfabeto (a-z)
        
        **Entrada:** Im√°genes de 28x28 p√≠xeles
        **Salida:** Probabilidades para cada letra del alfabeto
        """)

def speech_synthesis_interface(models):
    """Interfaz para s√≠ntesis de voz"""
    st.header("üîä S√≠ntesis de Voz")
    
    col1, col2 = st.columns([1, 1])
    
    with col1:
        st.subheader("üìù Generar Audio")
        
        # Input de texto
        text_input = st.text_input(
            "Escribe una palabra para sintetizar:",
            value="hola",
            help="Escribe cualquier palabra en espa√±ol"
        )
        
        # Par√°metros de s√≠ntesis
        st.subheader("‚öôÔ∏è Par√°metros de Voz")
        
        col_rate, col_pitch = st.columns(2)
        with col_rate:
            rate = st.slider("Velocidad", 50, 200, 80, help="Velocidad de habla (palabras por minuto)")
        with col_pitch:
            pitch = st.slider("Tono", 0, 100, 50, help="Altura del tono de voz")
        
        amplitude = st.slider("Volumen", 50, 200, 120, help="Amplitud del audio")
        
        if st.button("üéµ Generar Audio", type="primary"):
            if text_input.strip():
                with st.spinner("Generando audio..."):
                    try:
                        # Sintetizar audio
                        waveform = synthesize_word(
                            text_input.strip(),
                            rate=rate,
                            pitch=pitch,
                            amplitude=amplitude
                        )
                        
                        if waveform is not None:
                            with col2:
                                st.subheader("üéß Audio Generado")
                                
                                # Guardar audio temporal para reproducci√≥n
                                try:
                                    with tempfile.NamedTemporaryFile(delete=False, suffix='.wav') as tmp_file:
                                        if save_waveform_to_audio_file(waveform, tmp_file.name, WAV2VEC_SR):
                                            # Reproducir audio
                                            with open(tmp_file.name, 'rb') as audio_file:
                                                st.audio(audio_file.read(), format='audio/wav')
                                        else:
                                            st.warning("‚ö†Ô∏è No se pudo guardar el archivo de audio")
                                        
                                        os.unlink(tmp_file.name)  # Limpiar
                                except Exception as e:
                                    st.warning(f"‚ö†Ô∏è No se puede reproducir el audio: {str(e)}")
                                
                                # Mostrar waveform
                                fig = plot_waveform(waveform, f"Audio sintetizado: '{text_input}'")
                                st.pyplot(fig)
                                
                                # An√°lisis con TinyListener
                                st.subheader("üîç An√°lisis con TinyListener")
                                
                                device = models['device']
                                waveform_device = waveform.to(device)
                                
                                models['tiny_listener'].eval()
                                with torch.no_grad():
                                    logits, _ = models['tiny_listener']([waveform_device])
                                
                                # Predicci√≥n
                                predicted_idx = logits.argmax(dim=1).item()
                                predicted_word = models['words'][predicted_idx]
                                confidence = torch.softmax(logits, dim=1).max().item()
                                
                                if predicted_word.lower() == text_input.lower():
                                    st.success(f"‚úÖ ¬°Reconocido correctamente como '{predicted_word}'! (Confianza: {confidence:.2%})")
                                else:
                                    st.warning(f"‚ö†Ô∏è Reconocido como '{predicted_word}' (Confianza: {confidence:.2%})")
                                
                                # Top predicciones
                                probabilities = torch.softmax(logits, dim=1).squeeze().cpu().numpy()
                                top_indices = np.argsort(probabilities)[::-1][:3]
                                
                                st.write("**Top 3 predicciones:**")
                                for i, idx in enumerate(top_indices):
                                    word = models['words'][idx]
                                    prob = probabilities[idx]
                                    icon = "üéØ" if i == 0 else "üìç"
                                    st.write(f"{icon} {word} ({prob:.2%})")
                        
                        else:
                            st.error("‚ùå Error generando audio. ¬øTienes espeak instalado?")
                    
                    except Exception as e:
                        st.error(f"‚ùå Error en s√≠ntesis: {str(e)}")
            else:
                st.warning("‚ö†Ô∏è Por favor ingresa una palabra")
    
    # Informaci√≥n
    with st.expander("‚ÑπÔ∏è Informaci√≥n sobre S√≠ntesis"):
        st.markdown("""
        **S√≠ntesis de Voz** utiliza:
        - **espeak** para generar audio sint√©tico
        - Configuraci√≥n personalizable de velocidad, tono y volumen
        - An√°lisis autom√°tico con TinyListener para verificar calidad
        
        **Nota:** Requiere tener instalado `espeak` en el sistema:
        ```bash
        sudo apt-get install espeak  # Ubuntu/Debian
        brew install espeak         # macOS
        ```
        """)

if __name__ == "__main__":
    main()