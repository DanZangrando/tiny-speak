# ğŸ§  TinySpeak: Sistema Cognitivo Multimodal

**TinySpeak** es una plataforma de investigaciÃ³n y educaciÃ³n en Inteligencia Artificial que simula los procesos cognitivos humanos de **audiciÃ³n**, **visiÃ³n** e **imaginaciÃ³n**. Construido con PyTorch Lightning y Streamlit, ofrece una interfaz interactiva para entrenar, evaluar y experimentar con modelos de Deep Learning de vanguardia.

## âœ¨ CaracterÃ­sticas Principales

### ğŸ¤– Arquitectura Cognitiva Modular
El sistema se divide en tres "vÃ­as" o agentes especializados:

1.  **ğŸ‘‚ PhonologicalPathway (El OÃ­do)**
    *   **Modelo**: MelSpectrogram + Transformer Encoder.
    *   **FunciÃ³n**: Reconocimiento AutomÃ¡tico del Habla (ASR).
    *   **Capacidad**: Entiende palabras habladas y las mapea a conceptos.
    *   **InnovaciÃ³n**: Arquitectura ligera entrenada desde cero para eficiencia.

2.  **ğŸ‘ï¸ VisualPathway (La Vista)**
    *   **Modelo**: CNN + Linear Decoder.
    *   **FunciÃ³n**: Reconocimiento Ã“ptico de Caracteres (OCR).
    *   **Capacidad**: Lee letras manuscritas y tipografÃ­as variadas.
    *   **InnovaciÃ³n**: Simula la vÃ­a ventral del procesamiento visual humano.

3.  **ğŸ§  TinyReader (La Voz Interior)**
    *   **Modelo**: Transformer Decoder (Spelling-to-Audio).
    *   **FunciÃ³n**: ImaginaciÃ³n Auditiva.
    *   **Capacidad**: "Lee" una secuencia de letras y genera una alucinaciÃ³n auditiva (embedding) de cÃ³mo deberÃ­a sonar.
    *   **InnovaciÃ³n**: Entrenamiento con **PÃ©rdida Perceptual**, usando al *PhonologicalPathway* como juez para validar sus imaginaciones.

### ğŸ“Š AnalÃ­tica Avanzada e Interactiva
Cada modelo cuenta con un panel de control profesional:
*   **Curvas de Aprendizaje**: GrÃ¡ficos interactivos de pÃ©rdida y precisiÃ³n en tiempo real (Plotly).
*   **Predicciones en Vivo**: VisualizaciÃ³n animada de lo que el modelo "piensa" mientras entrena.
*   **Matrices de ConfusiÃ³n**: Mapas de calor para visualizar errores de clasificaciÃ³n.
*   **Espacio Latente 3D**: ProyecciÃ³n PCA interactiva para explorar cÃ³mo la IA organiza los conceptos.

### ğŸ”¬ Experimento de Transparencia
Un mÃ³dulo dedicado para validar la hipÃ³tesis cientÃ­fica del proyecto:
*   **Entrenamiento Multi-idioma**: EjecuciÃ³n automatizada de experimentos en EspaÃ±ol, InglÃ©s y FrancÃ©s.
*   **EvaluaciÃ³n Cruzada**: ComparaciÃ³n de rendimiento entre idiomas y modelos.
*   **Laboratorio Comparativo**: Prueba interactiva donde escribes una palabra y ves cÃ³mo cada "cerebro" (ES/EN/FR) la imagina y pronuncia.

## ğŸ—ï¸ Estructura del Proyecto

```
TinySpeak/
â”œâ”€â”€ ğŸ¯ app.py                    # Punto de entrada de la aplicaciÃ³n
â”œâ”€â”€ ğŸ§  models.py                 # Arquitecturas de redes neuronales (PyTorch)
â”œâ”€â”€ ğŸ”§ utils.py                  # Utilidades compartidas (audio, visualizaciÃ³n)
â”œâ”€â”€ 
â”œâ”€â”€ ğŸ“„ pages/                    # Interfaz de Usuario (Streamlit)
â”‚   â”œâ”€â”€ 01_ğŸ‘‚_Audio_Dataset.py   # GestiÃ³n de datos de audio
â”‚   â”œâ”€â”€ 02_ğŸ‘ï¸_Visual_Dataset.py  # GestiÃ³n de datos visuales
â”‚   â”œâ”€â”€ 03_ğŸ‘‚_Audio_Analytics.py # ExploraciÃ³n de datos
â”‚   â”œâ”€â”€ 05_ğŸ‘‚_TinyListener.py    # Entrenamiento y Lab: Listener
â”‚   â”œâ”€â”€ 06_ğŸ‘ï¸_VisualPathway.py   # Entrenamiento y Lab: Recognizer
â”‚   â”œâ”€â”€ 07_ğŸ‘ï¸ğŸ‘‚_TinyReader.py    # Entrenamiento y Lab: Reader
â”‚   â”œâ”€â”€ 08_ğŸ”¬_Transparency_Experiment.py # Experimento CientÃ­fico Automatizado
â”‚   â””â”€â”€ README.md                # DocumentaciÃ³n detallada de pÃ¡ginas
â”‚
â”œâ”€â”€ ğŸ“ components/               # Componentes UI reutilizables
â”‚   â”œâ”€â”€ analytics.py             # Motores de visualizaciÃ³n y mÃ©tricas
â”‚   â”œâ”€â”€ diagrams.py              # Generadores de diagramas de arquitectura
â”‚   â””â”€â”€ README.md                # DocumentaciÃ³n de componentes
â”‚
â”œâ”€â”€ ğŸ‹ï¸ training/                 # LÃ³gica de Entrenamiento (Lightning)
â”‚   â”œâ”€â”€ audio_module.py          # LightningModule: Listener
â”‚   â”œâ”€â”€ visual_module.py         # LightningModule: Recognizer
â”‚   â”œâ”€â”€ reader_module.py         # LightningModule: Reader
â”‚   â””â”€â”€ README.md                # DocumentaciÃ³n de entrenamiento
â”‚
â”œâ”€â”€ ğŸ“ models/                   # Checkpoints y metadatos guardados
â”œâ”€â”€ ğŸ“ data/                     # Datasets crudos y procesados
â””â”€â”€ ğŸ“ metrics/                  # Logs de entrenamiento (JSON)
```

## ğŸš€ InstalaciÃ³n y Uso

1.  **Clonar el repositorio**:
    ```bash
    git clone https://github.com/tu-usuario/tiny-speak.git
    cd tiny_speak
    ```

2.  **Crear entorno virtual** (Recomendado):
    ```bash
    python -m venv .venv
    source .venv/bin/activate  # Linux/Mac
    # .venv\Scripts\activate   # Windows
    ```

3.  **Instalar dependencias**:
    ```bash
    pip install -r requirements.txt
    ```

4.  **Ejecutar la aplicaciÃ³n**:
    ```bash
    streamlit run app.py
    ```

## ğŸ”¬ Fundamentos CientÃ­ficos
Este proyecto explora conceptos avanzados de IA:
*   **Self-Supervised Learning**: Uso de Wav2Vec 2.0.
*   **Transfer Learning**: AdaptaciÃ³n de modelos pre-entrenados a tareas especÃ­ficas.
*   **Multimodal Learning**: IntegraciÃ³n de visiÃ³n y audio en un espacio latente comÃºn.
*   **Generative AI**: CreaciÃ³n de representaciones sintÃ©ticas a partir de conceptos abstractos.

---
*Desarrollado con â¤ï¸ para la investigaciÃ³n en IA Cognitiva.*