# ğŸ¤ TinySpeak - Sistema Multimodal de IA

**TinySpeak** es un sistema completo de inteligencia artificial que combina reconocimiento de voz, visiÃ³n computacional y sÃ­ntesis de audio en una aplicaciÃ³n web moderna construida con Streamlit.

## âœ¨ CaracterÃ­sticas Principales

### ğŸ§  **Modelos de IA Integrados**
- **ğŸµ TinyListener**: Reconocimiento de palabras usando Wav2Vec2 + LSTM
- **ğŸ‘ï¸ TinyRecognizer**: Reconocimiento de letras manuscritas con CORnet-Z 

### ğŸ“š **GestiÃ³n Inteligente de Vocabularios**
- **Diccionarios Predefinidos**: Kalulu (espaÃ±ol), Phones (fonemas), temÃ¡ticos
- **Diccionarios Personalizados**: CreaciÃ³n de vocabularios especÃ­ficos
- **SincronizaciÃ³n AutomÃ¡tica**: ConfiguraciÃ³n centralizada y consistente

### ï¿½ **GeneraciÃ³n Avanzada de Audio**
- **SÃ­ntesis gTTS**: Google Text-to-Speech de alta calidad
- **Variaciones AutomÃ¡ticas**: 6 tipos (velocidad, tono, volumen, normalizado)
- **ConversiÃ³n WAV**: Procesamiento automÃ¡tico para compatibilidad
- **VerificaciÃ³n Inteligente**: ValidaciÃ³n automÃ¡tica de cada muestra

### ğŸ–¼ï¸ **GeneraciÃ³n de Datasets Visuales**
- **Letras SintÃ©ticas**: MÃºltiples fuentes y estilos tipogrÃ¡ficos
- **Variaciones Personalizables**: TamaÃ±os, efectos y transformaciones
- **Dataset Visual Completo**: Para entrenamiento de reconocimiento OCR

### âš¡ **Entrenamiento con PyTorch Lightning**
- **TinyListener Training**: Entrenamiento completo de reconocimiento de audio
- **TinyRecognizer Training**: Entrenamiento de reconocimiento visual
- **Callbacks Avanzados**: Early stopping, checkpoints y mÃ©tricas en tiempo real

## ï¿½ InstalaciÃ³n RÃ¡pida

### **Prerrequisitos**
- Python 3.8+ 
- Entorno virtual recomendado
- ConexiÃ³n a internet (para gTTS)

### **ConfiguraciÃ³n**
```bash
# 1. Clonar el proyecto
git clone [repository-url]
cd tiny_speak

# 2. Activar entorno virtual
source .venv/bin/activate  # Linux/macOS
# o .venv\Scripts\activate   # Windows

# 3. Instalar dependencias
pip install -r requirements.txt

# 4. Ejecutar aplicaciÃ³n
streamlit run app.py
```

La aplicaciÃ³n se abrirÃ¡ automÃ¡ticamente en `http://localhost:8501`

## ğŸ—ï¸ Arquitectura del Sistema

```
TinySpeak/
â”œâ”€â”€ ğŸ¯ app.py                    # AplicaciÃ³n principal Streamlit
â”œâ”€â”€ ğŸ§  models.py                 # Definiciones de modelos IA
â”œâ”€â”€ ğŸ”§ utils.py                  # Utilidades y helpers
â”œâ”€â”€ ğŸ“‹ requirements.txt          # Dependencias Python
â”œâ”€â”€ ğŸ“Š master_dataset_config.json # ConfiguraciÃ³n centralizada
â”œâ”€â”€ 
â”œâ”€â”€ ğŸ“ components/               # Componentes UI reutilizables
â”‚   â””â”€â”€ modern_sidebar.py        # Sidebar con glassmorphism
â”œâ”€â”€ 
â”œâ”€â”€ ğŸ“„ pages/                    # PÃ¡ginas de la aplicaciÃ³n
â”‚   â”œâ”€â”€ 01_ğŸµ_Audio_Dataset.py   # GestiÃ³n dataset audio
â”‚   â”œâ”€â”€ 02_ğŸ–¼ï¸_Visual_Dataset.py  # GestiÃ³n dataset visual
â”‚   â”œâ”€â”€ 03_ğŸµ_Audio_Analytics.py # AnalÃ­ticas audio
â”‚   â”œâ”€â”€ 04_ğŸ–¼ï¸_Visual_Analytics.py # AnalÃ­ticas visual
â”‚   â”œâ”€â”€ 05_ğŸµ_TinyListener.py    # Modelo TinyListener
â”‚   â””â”€â”€ 06_ğŸ–¼ï¸_TinyRecognizer.py  # Modelo TinyRecognizer
â”œâ”€â”€ 
â”œâ”€â”€ ğŸ‹ï¸ training/                 # MÃ³dulos de entrenamiento
â”‚   â”œâ”€â”€ audio_module.py          # TinyListener Lightning
â”‚   â”œâ”€â”€ visual_module.py         # TinyRecognizer Lightning
â”‚   â”œâ”€â”€ audio_dataset.py         # Datasets de audio
â”‚   â”œâ”€â”€ visual_dataset.py        # Datasets visuales
â”‚   â””â”€â”€ config.py               # ConfiguraciÃ³n datasets
â”œâ”€â”€ 
â”œâ”€â”€ ğŸ“ data/                     # Datasets descargados
â”œâ”€â”€ ğŸ“ checkpoints/              # Modelos entrenados
â”œâ”€â”€ ğŸ“ visual_dataset/           # ImÃ¡genes generadas
â””â”€â”€ ğŸ“ .streamlit/               # ConfiguraciÃ³n UI
```

## ğŸ’¡ Funcionalidades Detalladas

### ğŸµ **TinyListener - Reconocimiento de Audio**
- **Entrada MÃºltiple**: WAV, MP3, FLAC, M4A, grabaciÃ³n en vivo
- **AnÃ¡lisis Completo**: Waveform, espectrograma, predicciones
- **MÃ©tricas Avanzadas**: Confianza, logits, embeddings internos
- **Entrenamiento**: PyTorch Lightning con callbacks personalizados

### ğŸ‘ï¸ **TinyRecognizer - Reconocimiento Visual**  
- **Carga Flexible**: ImÃ¡genes manuscritas, sintÃ©ticas, fotografÃ­as
- **AnÃ¡lisis Visual**: Embeddings, mapas de atenciÃ³n, confianza
- **Entrenamiento Avanzado**: CORnet-Z backbone, augmentations automÃ¡ticas
- **EvaluaciÃ³n**: MÃ©tricas por clase, matriz de confusiÃ³n

### ğŸ¤ **Audio Dataset Manager**
- **SÃ­ntesis gTTS**: MÃºltiples idiomas y voces
- **Variaciones AutomÃ¡ticas**: Speed (0.8x-1.2x), pitch, volumen
- **Postprocesamiento**: NormalizaciÃ³n, padding, conversiÃ³n de formatos
- **ValidaciÃ³n**: ReproducciÃ³n automÃ¡tica y verificaciÃ³n de calidad

### ï¿½ï¸ **Visual Dataset Manager**
- **GeneraciÃ³n TipogrÃ¡fica**: 15+ fuentes, mÃºltiples tamaÃ±os
- **Augmentations**: RotaciÃ³n, ruido, blur, transformaciones afines
- **Balanceo AutomÃ¡tico**: DistribuciÃ³n equitativa por clase
- **ExportaciÃ³n**: PNG optimizado, metadatos JSON

### ğŸ“Š **Dashboard Analytics**
- **MÃ©tricas en Tiempo Real**: Estado de datasets, progreso de entrenamiento
- **Visualizaciones**: Plotly interactivo, mÃ©tricas dinÃ¡micas
- **Consistencia**: VerificaciÃ³n automÃ¡tica de sincronizaciÃ³n
- **Performance**: MÃ©tricas de modelos, comparaciones A/B

## ğŸ”§ ConfiguraciÃ³n Avanzada

### **Variables de Entorno**
```bash
# ConfiguraciÃ³n CUDA (opcional)
export CUDA_VISIBLE_DEVICES=0
export CUBLAS_WORKSPACE_CONFIG=:4096:8

# ConfiguraciÃ³n gTTS
export GTTS_LANG=es  # Idioma por defecto
```

### **ConfiguraciÃ³n Personalizada**
Edita `master_dataset_config.json` para personalizar:
- Vocabularios por defecto
- Rutas de datasets 
- ParÃ¡metros de generaciÃ³n
- Configuraciones de entrenamiento

## ğŸ› SoluciÃ³n de Problemas

### **Problemas Comunes**

| Error | Causa | SoluciÃ³n |
|-------|-------|----------|
| `ModuleNotFoundError: pytorch_lightning` | Dependencia faltante | `pip install pytorch-lightning` |
| `CUDA deterministic warning` | ConfiguraciÃ³n PyTorch | Ya corregido en v2.0+ |
| `gTTS network error` | ConexiÃ³n internet | Verificar conectividad |
| `Tensor size mismatch` | Audio padding | Collate function implementado |

### **Debugging**
```bash
# Verificar instalaciÃ³n
python -c "import torch, streamlit, transformers; print('OK')"

# Test modelo bÃ¡sico
python -c "from models import TinyListener; print('Models OK')"

# Verificar GPU
python -c "import torch; print(f'CUDA: {torch.cuda.is_available()}')"
```

## ğŸ“ˆ Roadmap

### **v2.1 - PrÃ³ximas Mejoras**
- [ ] **ExportaciÃ³n de Modelos**: ONNX, TorchScript, Hugging Face Hub
- [ ] **API REST**: Endpoints para inferencia programÃ¡tica
- [ ] **MÃ©tricas Avanzadas**: WandB integration, experiment tracking
- [ ] **Deployment**: Docker, cloud deployment automÃ¡tico

### **v3.0 - Funcionalidades Avanzadas**
- [ ] **Modelos Transformer**: Arquitecturas state-of-the-art
- [ ] **Multi-idioma**: Soporte completo para mÃºltiples idiomas
- [ ] **Real-time**: Inferencia en tiempo real optimizada
- [ ] **Federado**: Entrenamiento federado para privacidad

## ğŸ¤ Contribuciones

Las contribuciones son bienvenidas! Por favor:

1. **Fork** el repositorio
2. **Crea** una rama feature (`git checkout -b feature/nueva-funcionalidad`)
3. **Commit** los cambios (`git commit -am 'AÃ±adir nueva funcionalidad'`)
4. **Push** a la rama (`git push origin feature/nueva-funcionalidad`)
5. **Crea** un Pull Request

### **Ãreas de ContribuciÃ³n Prioritarias**
- ğŸ§  Nuevos modelos y arquitecturas
- ğŸ¨ Mejoras de UI/UX
- ğŸ“Š Nuevas mÃ©tricas y visualizaciones
- ğŸ”§ Optimizaciones de performance
- ğŸ“š DocumentaciÃ³n y tutoriales

## ğŸ“„ Licencia

Este proyecto estÃ¡ licenciado bajo **MIT License** - ver `LICENSE` para detalles.

## ğŸ™ Reconocimientos

- **Hugging Face** por los modelos pre-entrenados
- **PyTorch Lightning** por el framework de entrenamiento
- **Streamlit** por la plataforma web
- **Google** por gTTS y servicios de sÃ­ntesis

---

**Desarrollado con â¤ï¸ para la comunidad de IA multimodal**