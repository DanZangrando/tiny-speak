# ğŸ§  Arquitectura y OrganizaciÃ³n de TinySpeak

## ğŸ“‹ **Resumen Ejecutivo**

TinySpeak es un sistema de IA multimodal que implementa tres arquitecturas principales para reconocimiento de patrones en audio y visiÃ³n. La aplicaciÃ³n estÃ¡ organizada en pÃ¡ginas especializadas que permiten testing detallado de cada componente.

## ğŸ—ï¸ **Arquitecturas de Modelos**

### 1. ğŸµ **TinyListener (Audio â†’ Palabra)**

```python
# Arquitectura completa
Audio (16kHz WAV) 
    â†“
Wav2Vec2-Base-ES (Facebook)
- Modelo: facebook/wav2vec2-base-es-voxpopuli-v2  
- ParÃ¡metros: ~95M (congelados)
- Output: 768D embeddings a ~49Hz
    â†“
Feature Processing
- ExtracciÃ³n capa 5 de Wav2Vec2
- Downsampling factor 7
- Padding de secuencias variables
    â†“  
LSTM Network
- Input: 768 dim
- Hidden: 64 dim
- Layers: 2  
- Batch-first: True
    â†“
Linear Classifier  
- Input: 64 dim (Ãºltimo estado LSTM)
- Output: 200 clases (palabras espaÃ±ol)
```

**ParÃ¡metros entrenables:** ~592K
**Vocabulario:** 200 palabras en espaÃ±ol
**Input:** Audio WAV a 16kHz
**Output:** Probabilidades por palabra

---

### 2. ğŸ–¼ï¸ **TinyRecognizer (Imagen â†’ Letra)**

```python
# Arquitectura CORnet-Z (inspirada en cortex visual)
Image (28Ã—28Ã—3 RGB)
    â†“
V1 Block (Visual Area 1)
- Conv2d(3â†’64, kernel=7, stride=2)
- ReLU + MaxPool2d(3Ã—3, stride=2)  
    â†“
V2 Block (Visual Area 2)
- Conv2d(64â†’128, kernel=3)
- ReLU + MaxPool2d(3Ã—3, stride=2)
    â†“  
V4 Block (Visual Area 4)
- Conv2d(128â†’256, kernel=3)
- ReLU + MaxPool2d(3Ã—3, stride=2)
    â†“
IT Block (Inferotemporal Cortex) 
- Conv2d(256â†’512, kernel=3)
- ReLU + MaxPool2d(3Ã—3, stride=2)
    â†“
Decoder
- AdaptiveAvgPool2d(1Ã—1) 
- Flatten â†’ Linear(512â†’1024) â†’ ReLU
- Linear(1024â†’768) [Embedding space]
    â†“
Classifier
- Linear(768â†’26) [Letter classes a-z]
```

**ParÃ¡metros entrenables:** ~2.1M
**Input:** ImÃ¡genes RGB 28Ã—28
**Output:** Probabilidades para 26 letras (a-z)
**Embedding:** Espacio de 768 dimensiones

---

### 3. ğŸ”— **TinySpeller (Multimodal: VisiÃ³n + Audio)**

```python
# Arquitectura combinada
Secuencia ImÃ¡genes [L1, L2, ..., Ln]
    â†“
TinyRecognizer (para cada letra)
- Output: Embedding 768D por letra
    â†“  
Secuencia Embeddings [E1, E2, ..., En]
    â†“
TinySpeak (LSTM compartido)
- Input: Secuencia de embeddings 768D
- LSTM(768, 64, num_layers=2)  
- Output: Clasificador(64â†’200 palabras)
```

**Componentes:**
- TinyRecognizer (congelado): Extrae embeddings visuales
- TinySpeak (entrenado): Procesa secuencias para palabras

**Casos de uso:**
1. Secuencia letras manuscritas â†’ palabra completa
2. ComparaciÃ³n con reconocimiento de audio directo

## ğŸ“ **OrganizaciÃ³n de la AplicaciÃ³n**

### **PÃ¡gina Principal** (`app.py`)
- ğŸ  **Dashboard general** del sistema
- ğŸ“Š **Estado de componentes** (dispositivo, vocabulario, espeak)
- ğŸ§ª **Test rÃ¡pido** del sistema completo
- ğŸ§­ **NavegaciÃ³n** a pÃ¡ginas especializadas

### **PÃ¡ginas Especializadas** (`pages/`)

#### 1. ğŸµ **TinyListener** (`01_ğŸµ_TinyListener.py`)
**Funcionalidades:**
- ğŸ“ **Carga de archivos** (WAV, MP3, FLAC, M4A)
- ğŸ¤ **GrabaciÃ³n en tiempo real** 
- ğŸ”Š **SÃ­ntesis + reconocimiento** (test loop cerrado)
- ğŸ“Š **AnÃ¡lisis interno** (arquitectura, vocabulario, embeddings)

**Testing incluido:**
- Upload y anÃ¡lisis de archivos de audio
- GrabaciÃ³n directa desde micrÃ³fono
- SÃ­ntesis controlada con parÃ¡metros (velocidad, tono, volumen)
- AnÃ¡lisis de palabras especÃ­ficas del vocabulario
- VisualizaciÃ³n de waveforms y distribuciones de logits

#### 2. ğŸ–¼ï¸ **TinyRecognizer** (`02_ğŸ–¼ï¸_TinyRecognizer.py`)
**Funcionalidades:**
- ğŸ“ **Carga de imÃ¡genes** de letras manuscritas
- âœï¸ **GeneraciÃ³n de letras** sintÃ©ticas (prÃ³ximamente: canvas)
- ğŸ”¬ **Test sistemÃ¡tico** del alfabeto completo
- ğŸ“Š **AnÃ¡lisis interno** (activaciones por capa, embeddings)

**Testing incluido:**
- Reconocimiento de letras individuales
- Test sistemÃ¡tico A-Z con mÃ©tricas de precisiÃ³n
- VisualizaciÃ³n de activaciones internas (V1, V2, V4, IT)
- AnÃ¡lisis de embeddings de 768 dimensiones
- Matriz de confusiÃ³n para errores

#### 3. ğŸ”— **TinySpeller** (`03_ğŸ”—_TinySpeller.py`)
**Funcionalidades:**
- ğŸ–¼ï¸â¡ï¸ğŸ“ **Secuencia letras â†’ palabra** 
- ğŸµâ¡ï¸ğŸ“ **Audio directo â†’ palabra**
- âš–ï¸ **ComparaciÃ³n multimodal** (visiÃ³n vs audio)
- ğŸ”¬ **AnÃ¡lisis avanzado** (arquitectura, embeddings, benchmarks)

**Testing incluido:**
- GeneraciÃ³n automÃ¡tica de secuencias de letras
- AnÃ¡lisis comparativo entre modalidades
- Benchmark de rendimiento (latencia por modalidad)
- AnÃ¡lisis de consenso/discrepancia entre modelos
- ExploraciÃ³n de espacios de embeddings

## ğŸ› ï¸ **Archivos de Soporte**

### **Modelos** (`models.py`)
```python
# Clases principales
- TinySpeak: LSTM + Classifier base
- TinyListener: Wav2Vec2 + TinySpeak  
- TinyRecognizer: CORnet-Z + Classifier
- TinySpeller: TinyRecognizer + TinySpeak
- CORnet_Z: Arquitectura visual cortical
```

### **Utilidades** (`utils.py`)
```python
# Funciones clave
- encontrar_device(): DetecciÃ³n GPU/CPU/MPS
- load_wav2vec_model(): Carga Wav2Vec2 con config
- load_waveform(): Carga audio con fallback librosa
- synthesize_word(): GeneraciÃ³n con espeak
- plot_waveform(): VisualizaciÃ³n de audio
- plot_logits(): GrÃ¡ficos de predicciones
- ensure_data_downloaded(): Descarga automÃ¡tica datasets
```

### **Testing** (`test_setup.py`)
```python
# Verificaciones del sistema
- test_imports(): PyTorch, Transformers, Streamlit
- test_device(): DetecciÃ³n y configuraciÃ³n dispositivo  
- test_espeak(): SÃ­ntesis de voz funcional
- test_models(): Carga e inicializaciÃ³n modelos
```

## ğŸ“Š **Datasets y Datos**

### **Datasets Descargados AutomÃ¡ticamente:**
1. **tiny-kalulu-200**: 200 palabras espaÃ±ol (train/val)
2. **tiny-phones-200**: Fonemas concatenados 
3. **tiny-emnist-26**: Letras manuscritas A-Z

### **Estructura de Datos:**
```
data/
â”œâ”€â”€ tiny-kalulu-200/
â”‚   â”œâ”€â”€ train/[palabra]/[archivos.wav]
â”‚   â””â”€â”€ val/[palabra]/[archivos.wav]  
â”œâ”€â”€ tiny-phones-200/
â”‚   â””â”€â”€ val/[fonema]/[archivos.wav]
â””â”€â”€ tiny-emnist-26/
    â”œâ”€â”€ train/[letra]/[imÃ¡genes.JPEG] 
    â””â”€â”€ val/[letra]/[imÃ¡genes.JPEG]
```

## ğŸ¯ **Flujos de Testing Recomendados**

### **1. Test Individual de Modelos:**
```
ğŸµ TinyListener â†’ Cargar audio â†’ Verificar predicciÃ³n
ğŸ–¼ï¸ TinyRecognizer â†’ Cargar letra â†’ Verificar reconocimiento  
ğŸ”— TinySpeller â†’ Secuencia letras â†’ Verificar palabra
```

### **2. Test de Consistencia:**
```
Palabra "casa" â†’ SÃ­ntesis â†’ TinyListener â†’ Â¿Reconoce "casa"?
Letras C-A-S-A â†’ TinySpeller â†’ Â¿Predice "casa"?
```

### **3. Test Comparativo:**
```
Misma palabra por mÃºltiples modalidades â†’ Â¿Consenso?
Audio vs VisiÃ³n â†’ Â¿Misma predicciÃ³n?  
```

### **4. Test de Robustez:**
```
ParÃ¡metros sÃ­ntesis variados â†’ Â¿Estabilidad?
Letras con diferentes estilos â†’ Â¿GeneralizaciÃ³n?
```

## ğŸš€ **Extensiones Futuras**

### **Mejoras de UI:**
- Canvas interactivo para dibujo de letras
- GrabaciÃ³n de audio con control de calidad
- VisualizaciÃ³n 3D de embeddings
- Dashboard de mÃ©tricas en tiempo real

### **Mejoras de Modelos:**
- Fine-tuning de Wav2Vec2 en dominio especÃ­fico
- Aumento de datos para TinyRecognizer  
- Arquitectura attention para TinySpeller
- Modelos mÃ¡s grandes con mejor precisiÃ³n

### **Nuevas Funcionalidades:**
- Reconocimiento de palabras fuera del vocabulario
- DetecciÃ³n de idioma automÃ¡tica
- SÃ­ntesis de voz con mÃºltiples voces
- API REST para integraciÃ³n externa

## ğŸ“ˆ **MÃ©tricas de Rendimiento Actuales**

### **TinyListener:**
- Vocabulario: 200 palabras espaÃ±ol
- Latencia: ~100-200ms por audio
- PrecisiÃ³n: Depende de calidad audio

### **TinyRecognizer:** 
- Clases: 26 letras (a-z)
- Latencia: ~10-20ms por imagen
- PrecisiÃ³n: Alta en letras claras

### **TinySpeller:**
- Modalidades: 2 (visiÃ³n + audio)
- ComparaciÃ³n: AnÃ¡lisis de consenso
- Versatilidad: Palabras de longitud variable

---

*Esta arquitectura permite testing exhaustivo y comprensiÃ³n profunda de cada componente del sistema TinySpeak, facilitando tanto la investigaciÃ³n como la demostraciÃ³n de capacidades multimodales.*